{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from geopy import geocoders\n",
    "# import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5621065",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# loading pickled dataframes, making new dataframes out of them and saving them to the local drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d90985",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def pickled_df_loader (year, columns):\n",
    "#     \"\"\"\n",
    "#     loads dataframes from the pickled dfs. \n",
    "#     \"\"\"\n",
    "#     df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_1\")[columns]\n",
    "#     df= pd.concat ([df, pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_2\")[columns]]\\\n",
    "#                    , axis=0)\n",
    "#     df= pd.concat ([df, pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_3\")[columns]]\\\n",
    "#                    , axis=0)\n",
    "    \n",
    "#     df['publication_date']= df['publication_date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "#     df['publication_date'] = pd.to_datetime(df['publication_date'], format='%Y-%m-%d')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a91b57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def challenge_df_generator (begin, end):\n",
    "#     \"\"\"\n",
    "#     loads all pickled dataframes belonging to the years in the year range and returns their concatenation as a single df. \n",
    "#     \"\"\" \n",
    "    \n",
    "#     columns= ['paper_title', 'publication_date', 'place_of_distribution']\n",
    "    \n",
    "#     df= pickled_df_loader (begin, columns)\n",
    "    \n",
    "#     for year in range (begin+1, end+1):\n",
    "#         df= pd.concat([df, pickled_df_loader(year, columns)], axis=0)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe922d6c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyzing the dataframe containing all the data between 1914 and 1945:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4a1ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# newspapers_1914_1945_df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_no_article_14_45\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3fb4c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b06ca92",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a636124",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b85348",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's see what we can do with the lists of the cities: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301e8e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting all places of distribution and storing them in a pickled python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e0bd5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# unique_cities=[]\n",
    "# for city in newspapers_1914_1945_df['place_of_distribution']:\n",
    "#     if city not in unique_cities: \n",
    "#         unique_cities.append(city)\n",
    "        \n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/unique_cities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(unique_cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dc992",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# city_list= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/unique_cities.pkl\")\n",
    "# city_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa8690",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting names of single cities from the pickled city list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9722b07",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single_city_list= []\n",
    "\n",
    "# for place in city_list:\n",
    "#     if isinstance (place, list):\n",
    "#         for i in range(0, len(place)):\n",
    "#             if place[i] not in single_city_list:\n",
    "#                 single_city_list.append(place[i])\n",
    "#     else: \n",
    "#         if place not in city_list:\n",
    "#             city_list.append(place)\n",
    "\n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(single_city_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8210913",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cities_1914_1945= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\")\n",
    "# cities_1914_1945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300f1c8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pinning the cities from a city list on the world map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ae075",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def map_maker(city_list):\n",
    "# #plots the cities in the city list on a map.\n",
    "    \n",
    "#     city_dict= {}\n",
    "#     gn= geocoders.GeoNames(username=\"golisf\")\n",
    "    \n",
    "#     for city in city_list:\n",
    "#         if not pd.isna(city):\n",
    "#             city_dict[city]= gn.geocode(city)\n",
    "            \n",
    "#     # Create a map centered at a location, you can adjust the coordinates and zoom level as needed\n",
    "#     map_center= [51.1657, 10.4515]  # Germany's approximate center\n",
    "#     my_map= folium.Map(location=map_center, zoom_start=6)\n",
    " \n",
    "#     # Add markers for each city\n",
    "#     for city in list(city_dict.keys()):\n",
    "#         folium.Marker(location=city_dict[city], popup=city).add_to(my_map)\n",
    "        \n",
    "#     return my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0357122",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cities_1914_1945= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\")\n",
    "# map_maker(cities_1914_1945)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea123aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's see what we can do with the paper titles and publication years: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c825bc8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting the titles of available newspapers and storing them in a pickled python list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429bf75",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# unique_papers=[]\n",
    "# for paper in newspapers_1914_1945_df['paper_title']:\n",
    "#     if paper not in unique_papers: \n",
    "#         unique_papers.append(paper)\n",
    "        \n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/unique_papers.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(unique_papers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38498da8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_papers= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/unique_papers.pkl\")\n",
    "# all_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89289b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a df with the following columns: \n",
    "\n",
    "- name of newspaper\n",
    "- cities of distribution\n",
    "- date begin\n",
    "- date end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6b8e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df = newspapers_1914_1945_df.groupby('paper_title').agg(\n",
    "#     publication_begin=('publication_date', 'min'),\n",
    "#     publication_end=('publication_date', 'max'),\n",
    "#     place_of_distribution=('place_of_distribution', 'first')\n",
    "# ).reset_index()\n",
    "\n",
    "# paper_publication_df.to_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52689",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Running the above code, I realized that the same newspaper sometimes appears multiple times in the title, each time with minor changes (eg. issues and numbers are added to the title). So I optimized the code as below. In the below code, the newspaper title is tokenized and the the first three tokens of the title are added to the df as a separate column. Then the newspapers are grouped by the first three tokens and the newspaper_title is replaced by the shortest of newspaper title that shares the first three tokens. \n",
    "\n",
    "Question: Could it be that the different versions of the same newspaper have been distributed in different cities? \n",
    "\n",
    "- Perhaps the cities should not play an important role in the challegens. \n",
    "- Perhaps I have to write a more sophisticated code that combines all distribution places of all the versions of a single newspaper into a longer list and adds this list to the final paper_publication_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9199cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Function to extract first two tokens from a string\n",
    "# def extract_tokens(title):\n",
    "#     tokens = title.split()\n",
    "#     return ' '.join(tokens[:2])\n",
    "\n",
    "# # Apply the function to create a new column for the first three tokens\n",
    "# newspapers_1914_1945_df['first_two_tokens'] = newspapers_1914_1945_df['paper_title'].apply(extract_tokens)\n",
    "\n",
    "# # Group by the first three tokens and aggregate to get the shortest title\n",
    "# paper_publication_df = newspapers_1914_1945_df.groupby('first_two_tokens').agg(\n",
    "#     paper_title=('paper_title', lambda x: min(x, key=len)),\n",
    "#     publication_begin=('publication_date', 'min'),\n",
    "#     publication_end=('publication_date', 'max'),\n",
    "#     place_of_distribution=('place_of_distribution', 'first')\n",
    "# ).reset_index()\n",
    "\n",
    "# # Drop the temporary column\n",
    "# paper_publication_df.drop(columns='first_two_tokens', inplace=True)\n",
    "\n",
    "# paper_publication_df.to_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837a6f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see which newspapers were published between which years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f7353",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")\n",
    "# paper_publication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f5e8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61931e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how many \"Fachzeitschriften\" there are among the newspapers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26ee23",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fach_papers= [paper for paper in paper_publication_df['paper_title'] if 'fach' in paper.lower()]\n",
    "# paper_publication_df[paper_publication_df['paper_title'].isin(fach_papers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586ead9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see for how long each newspaper has been published after 1914:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef812f40",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# fig, ax= plt.subplots(figsize=(30, 15))\n",
    "# ax.set_title('Publication span of each newspaper.' ,fontsize=25)\n",
    "\n",
    "# for i, row in paper_publication_df.iterrows():\n",
    "#     bar_length= (row['publication_end'].year - row['publication_begin'].year)\n",
    "    \n",
    "#     if bar_length>0:\n",
    "#         ax.barh(row['paper_title'][0:50], bar_length, color='g', left=row['publication_begin'].year)\n",
    "#     else:\n",
    "#         #the short red bars represent newspapers that were only published within a year. \n",
    "#         ax.barh(row['paper_title'][0:50], 0.1, color='r', left=row['publication_begin'].year)\n",
    "    \n",
    "# ax.set_xlim (1913, 1946)\n",
    "# ax.tick_params(axis=\"x\", bottom=True, top=True, labelbottom=True, labeltop=True)\n",
    "# #ax.xaxis.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "# plt.xticks(ticks=np.arange(1914, 1946), rotation=45)\n",
    "# plt.rcParams['xtick.labelsize'] = 15\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca5612",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "1. Interestingly, the publication of a considerable amount of papers seems to have stopped in 1933.\n",
    "2. Despite applying the function that extracted the first three tokens from the newspaper titles, there are still instances of the same newspaper with two different titles, one shorter and one longer. More cleaning has to be done on the titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de810239",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Doing PoS tagging and NER on example articles, using spaCy and network analysis using NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a768f6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51a052e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df43b242",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_1931_part_1\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5342323",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Are there any \"Fachzeitschriften\" in df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08245821",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# fach_papers= [paper for paper in df['paper_title'] if 'fach' in paper.lower()]\n",
    "# df[df['paper_title'].isin(fach_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6aa558",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random_df=df.sample(3)\n",
    "# random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422e03eb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# random_df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df34190",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# text_1= random_df.iloc[0,9]\n",
    "# text_2= random_df.iloc[1,9]\n",
    "# text_3= random_df.iloc[2,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c3e65",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Here is the link to the page in which the text_1 appears: https://www.deutsche-digitale-bibliothek.de/newspaper/item/B57SF7JW24X55ZNNO5AENGAPSBQLHFYC?tx_dlf[highlight_word]=Neue%2BMannheimer%2BZeitung&issuepage=8&query=Neue+Mannheimer+Zeitung&fromDay=2&fromMonth=3&fromYear=1931&toDay=2&toMonth=3&toYear=1931&hit=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231adb3",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #! python -m spacy download de_core_news_sm\n",
    "# nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# doc_1 = nlp(text_1)\n",
    "# sentences_1 = list(doc_1.sents)\n",
    "\n",
    "# doc_2 = nlp(text_2)\n",
    "# sentences_2 = list(doc_2.sents)\n",
    "\n",
    "# doc_3 = nlp(text_3)\n",
    "# sentences_3 = list(doc_3.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ad4a0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "PoS (Part of Speech) tagging in the first sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40992de8",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for token in sentences_1[0]:\n",
    "#     print('{}: {}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce192872",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A function that extracts all nouns from all sentences within a given text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52b07d1",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def extract_nouns(text):\n",
    "    \n",
    "#     doc = nlp(text)\n",
    "#     sentences = list(doc.sents)\n",
    "#     entities=[]\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         sentence_entities = []\n",
    "#         for token in sentence:\n",
    "#             if token.pos_ == 'NOUN':\n",
    "#                 sentence_entities.append(token.text)\n",
    "    \n",
    "#         if len(sentence_entities)>0:\n",
    "#             entities.append(sentence_entities)\n",
    "    \n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd746d33",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nouns_1= extract_nouns(text_1)\n",
    "# nouns_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda54cf0",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Coverting noun lists into network data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fc194",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def get_network_data(nouns):\n",
    "#     final_sources = []\n",
    "#     final_targets = []\n",
    "#     for row in nouns:\n",
    "#         source = row[0]\n",
    "#         targets = row[1:]\n",
    "#         for target in targets:\n",
    "#             final_sources.append(source)\n",
    "#             final_targets.append(target)\n",
    "#     df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee0927",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network_df_1= get_network_data(nouns_1).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb4d3e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Converting network data into visualized networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c26b91",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# G_1 = nx.from_pandas_edgelist(network_df_1)\n",
    "# nx.info(G_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b7ba0",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5, edge_length=100):\n",
    "    \n",
    "#     import numpy as np\n",
    "#     from IPython.display import SVG\n",
    "#     from sknetwork.visualization import svg_graph\n",
    "#     from sknetwork.data import Bunch\n",
    "#     from sknetwork.ranking import PageRank\n",
    "    \n",
    "# #     # Compute force-directed layout\n",
    "# #     pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "#     adjacency = nx.to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight='weight', format='csr')\n",
    "#     names = np.array(list(G.nodes()))\n",
    "#     graph = Bunch()\n",
    "#     graph.adjacency = adjacency\n",
    "#     graph.names = np.array(names)\n",
    "#     pagerank = PageRank()\n",
    "#     scores = pagerank.fit_transform(adjacency)\n",
    "#     if show_names:\n",
    "#         image = svg_graph(graph.adjacency, font_size=font_size,\n",
    "#                           node_size=node_size, names=graph.names, width=700, height=500,\n",
    "#                           scores=scores, edge_width=edge_width)\n",
    "#     else:\n",
    "#         image = svg_graph(graph.adjacency, node_size=node_size,\n",
    "#                           width=700, height=500, scores = scores, edge_width=edge_width, edge_length=edge_length)\n",
    "    \n",
    "#     return SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e53ae89",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw_graph(G_1, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc114aed",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nouns_2= extract_nouns(text_2)\n",
    "# network_df_2= get_network_data(nouns_2).head(40)\n",
    "# G_2 = nx.from_pandas_edgelist(network_df_2)\n",
    "# draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6d1a0e",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nouns_3= extract_nouns(text_3)\n",
    "# network_df_3= get_network_data(nouns_3).head(40)\n",
    "# G_3 = nx.from_pandas_edgelist(network_df_3)\n",
    "# draw_graph(G_3, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95089936",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<font color=red>RESULT:</font> Finding nouns in the newspaper data is useless, because there are lots of them and some of them are not relevant. Except we search for certain nouns such as \"Nazis\" or \"Nationalsozialisten\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da1920",
   "metadata": {
    "hidden": true
   },
   "source": [
    "NER (Named Entity Recognition) with spaCy in the first article: 👎🏽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12778892",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# for ent in doc_2.ents:\n",
    "#     print('{}: {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3f5d8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8585f5a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64855d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sentences_1 = sent_tokenize(text_1)\n",
    "# sentences_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcb1a73",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence = sentences_1[3]\n",
    "\n",
    "# from nltk.tokenize import casual_tokenize\n",
    "# tokens = casual_tokenize(sentence)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bb7cc",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ed936",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from string import punctuation\n",
    "\n",
    "# def extract_entities(sentence):\n",
    "#     \"\"\"\n",
    "#     extracts entities from single sentences.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     entities = []\n",
    "#     tokens = casual_tokenize(sentence)\n",
    "    \n",
    "#     for row in nltk.pos_tag(tokens):\n",
    "#         token = row[0]\n",
    "#         tag = row[1]\n",
    "#         if tag == 'NNP':\n",
    "#             for p in punctuation:\n",
    "#                 if p in token:\n",
    "#                     cutoff = token.index(p)\n",
    "#                     token = token[:cutoff]\n",
    "#             if len(token) > 1:\n",
    "#                 entities.append(token)\n",
    "            \n",
    "#     if len(entities) > 0:\n",
    "#         return entities\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc98f2",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# entities = [extract_entities(sentence) for sentence in sentences_1]\n",
    "# len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0c385",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame({'sentence':sentences_1, 'entities':entities})\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a472dfe",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df = df.dropna()\n",
    "# df = df[df['entities'].apply(len)>1]\n",
    "# entities = df['entities'].to_list()\n",
    "# len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97537231",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def get_text_entities(text):\n",
    "    \n",
    "#     sentences = sent_tokenize(text)\n",
    "#     entities = [extract_entities(sentence) for sentence in sentences]\n",
    "#     df = pd.DataFrame({'sentence':sentences, 'entities':entities})\n",
    "#     df = df.dropna()\n",
    "#     df = df[df['entities'].apply(len) > 1]\n",
    "#     entities = df['entities'].to_list()\n",
    "    \n",
    "#     german_stop_words = stopwords.words('german')\n",
    "    \n",
    "#     for list in entities: \n",
    "#         for item in list:\n",
    "#             if item.lower() in german_stop_words:\n",
    "#                 list.remove(item)\n",
    "    \n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56912e2a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get_text_entities (text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e28d7",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get_text_entities (text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957f383",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# get_text_entities (text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a529b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# network_df_1= get_network_data(get_text_entities (text_1))\n",
    "# network_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adae8d",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# G_1 = nx.from_pandas_edgelist(network_df_1)\n",
    "# draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d0be36",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network_df_2= get_network_data(nouns_2).head(40)\n",
    "# G_2 = nx.from_pandas_edgelist(network_df_2)\n",
    "# draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51378547",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network_df_3= get_network_data(nouns_3).head(40)\n",
    "# G_3 = nx.from_pandas_edgelist(network_df_3)\n",
    "# draw_graph(G_3, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c0015a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#nouns_1= extract_nouns(text_1)\n",
    "# nouns_2= extract_nouns(text_2)\n",
    "# nouns_3= extract_nouns(text_3)\n",
    "\n",
    "#len(nouns_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bfa13",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(get_text_entities (text_1))\n",
    "# get_text_entities (text_2)\n",
    "# get_text_entities (text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5cf8f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## RESULT: spaCy performs much better than NLTK in extracting nouns and needs less cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf38c2",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exploring the possibility of a challenge: Looking for War!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4eead7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting only dataframes containing a certain list of words in their full text and saving them to pickled dataframes on the local drive again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4db1bf",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The following cells load newspaper dfs and searche their full text for the word 'krieg'. If the word is in the full text in each row, the row is kept. Otherwise, it is deleted. The resulting df is saved to a pickled file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f42d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def pickled_df_loader (year, part, columns, words):\n",
    "#     \"\"\"\n",
    "#     loads all three dataframes from the pickled dfs for each year and concatenates them. \n",
    "#     \"\"\"\n",
    "#     df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_{part}\")[columns]\n",
    "    \n",
    "# #     for index, row in df.iterrows():\n",
    "# #         if not any(word in row['plainpagefulltext'].lower() for word in words):\n",
    "# #             df.drop(index, inplace=True)\n",
    "    \n",
    "#     # Convert 'plainpagefulltext' to lowercase and check for presence of words using vectorized operations\n",
    "#     # | (pipe) is a regular expression standing for \"or\".\n",
    "#     mask = df['plainpagefulltext'].str.lower().str.contains('|'.join(words))\n",
    "#     df = df[mask]\n",
    "    \n",
    "#     df['publication_date']= df['publication_date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "#     df['publication_date'] = pd.to_datetime(df['publication_date'], format='%Y-%m-%d')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07dd4c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# columns= ['paper_title', 'publication_date', 'place_of_distribution', 'plainpagefulltext', 'pagenumber']\n",
    "# words=['krieg']\n",
    "\n",
    "# for year in range(1914, 1946):\n",
    "#     for part in range(1,4):\n",
    "#         df= pickled_df_loader (year, part, columns, words)\n",
    "#         df.to_pickle (f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_{part}_krieg\")\n",
    "#         print(f\"task for {year} part {part} complete. length= {len(df)}\")\n",
    "#     print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af5a11",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let us load one of the dataframes with the word \"krieg\" in all of its full texts and see what a network analysis of the word would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c2d6ec",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b940e942",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/full_text_dfs_krieg/newspapers_ger_1940_part_1_krieg\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d755c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df['paper_title'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b978637",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df[df['paper_title'].apply(lambda x: 'Stuttgarter neues Tagblatt' in x)].iloc[0,0]\n",
    "title= 'Stuttgarter NS-Kurier : Bauorgan der NSDAP : Stuttgarter neues Tagblatt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3104cd0a",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hamburger_df=df[df['paper_title']==title]\n",
    "hamburger_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed9d2b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The newspaper \"Hamburger Fremdblatt\" seems to have the highest amount of articles in this df. I want to see how this particular newspaper has been reporting about war in the first four months of 1914. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15515e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let us tokenize sentences and words and filter out every instance of 'krieg':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52a637",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8983f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_sentences(text, word):\n",
    "    \"\"\"\n",
    "    extracts all sentences that contain the word \"krieg\".\n",
    "    \"\"\" \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentences_with_word=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if word in str(sentence).lower():\n",
    "                sentences_with_word.append(sentence)\n",
    "            \n",
    "    return sentences_with_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2933a",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text=hamburger_df.iloc[20,-2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b97f50",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "extract_sentences(text, 'krieg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba76fd34",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sents_with_krieg = hamburger_df.iloc[:, -2].apply(lambda x: extract_sentences(x, 'krieg')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b13e98",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sents_with_krieg[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39af6863",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Interesting! In all of the reports of this newspaper, the word \"krieg\" appears only in one sentence per article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607f975c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def extract_nouns(sentence_list):\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        sentence_entities = []\n",
    "        for token in sentence[0]:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                sentence_entities.append(token.text)\n",
    "    \n",
    "        if len(sentence_entities) > 0:\n",
    "            entities.append(sentence_entities)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f9620",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nouns= extract_nouns(sents_with_krieg)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04e9ed1",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Filtering out lists that begin with a noun containing 'kireg'. These are the sentences that directly thematize a war:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf5cb6c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "new_nouns= []\n",
    "\n",
    "for row in nouns:\n",
    "    if 'krieg' in row[0].lower():\n",
    "        new_nouns.append(row)\n",
    "    \n",
    "new_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef263148",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_network_data(nouns):\n",
    "    \"\"\"\n",
    "    converting a list of nouns into network data.\n",
    "    \"\"\"\n",
    "    \n",
    "    final_sources = []\n",
    "    final_targets = []\n",
    "    \n",
    "    for row in nouns:\n",
    "        source = row[0]\n",
    "        targets = row[1:]\n",
    "        for target in targets:\n",
    "            final_sources.append(source)\n",
    "            final_targets.append(target)\n",
    "    df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60dff6fc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def draw_graph(G, show_names=True, node_size=1, font_size=10, edge_width=0.5):\n",
    "    \n",
    "    import numpy as np\n",
    "    from IPython.display import SVG\n",
    "    from sknetwork.visualization import svg_graph\n",
    "    from sknetwork.data import Bunch\n",
    "    from sknetwork.ranking import PageRank\n",
    "    \n",
    "    adjacency = nx.to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight='weight', format='csr')    \n",
    "    names = np.array(list(G.nodes()))\n",
    "    graph = Bunch()\n",
    "    graph.adjacency = adjacency\n",
    "    graph.names = np.array(names)\n",
    "    pagerank = PageRank()\n",
    "    scores = pagerank.fit_transform(adjacency)\n",
    "    if show_names:\n",
    "        image = svg_graph(graph.adjacency, font_size=font_size,\n",
    "                          node_size=node_size, names=graph.names, width=700, height=700,\n",
    "                          scores=scores, edge_width=edge_width)\n",
    "    else:\n",
    "        image = svg_graph(graph.adjacency, node_size=node_size,\n",
    "                          width=700, height=700, scores = scores, edge_width=edge_width)\n",
    "    \n",
    "    return SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62985265",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "n_data=get_network_data(new_nouns).head(50)\n",
    "G_hamburger = nx.from_pandas_edgelist(n_data)\n",
    "draw_graph (G_hamburger, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de0a200",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c11800",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    \n",
    "    # Determine sentiment based on compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return sentiment, sentiment_scores\n",
    "\n",
    "# Analyze sentiment\n",
    "sentiment, sentiment_scores = analyze_sentiment(text)\n",
    "\n",
    "# Print results\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Sentiment Scores:\", sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2406e521",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So, what we have done so far: \n",
    "\n",
    "1. we filtered out newspaper articles from the first four months of 1914, in which the word \"krieg\" appears. \n",
    "2. we filtered out articles from hamburger fremdenblatt, because it was the most present newspaper in this time. \n",
    "3. we filtered out single sentences from these articles containing the word \"krieg\", with the question in mind: how has hamburger fremdenblatt reported about war in the first four months of 1914?\n",
    "4. we created a list of nouns or other words containing \"krieg\" from these sentences. \n",
    "5. we converted this list into network data.\n",
    "6. we visualized 50 rows of the network data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8f8cc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis and Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fd501e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f542dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>place_of_distribution</th>\n",
       "      <th>plainpagefulltext</th>\n",
       "      <th>pagenumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Schwerter Zeitung : Heimatblatt für die Stadt ...</td>\n",
       "      <td>1925-04-18</td>\n",
       "      <td>[Schwerte, Westhofen (Kreis Iserlohn), Schwert...</td>\n",
       "      <td>besonders stark leidenden Magerkohlenzechen be...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Schwerter Zeitung : Heimatblatt für die Stadt ...</td>\n",
       "      <td>1925-04-18</td>\n",
       "      <td>[Schwerte, Westhofen (Kreis Iserlohn), Schwert...</td>\n",
       "      <td>sung von 8000 . r auf meine An e wäre völlig e...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Schwerter Zeitung : Heimatblatt für die Stadt ...</td>\n",
       "      <td>1925-04-18</td>\n",
       "      <td>[Schwerte, Westhofen (Kreis Iserlohn), Schwert...</td>\n",
       "      <td>* Im Interesse des ganzen Volkes liegt es , da...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Schwerter Zeitung : Heimatblatt für die Stadt ...</td>\n",
       "      <td>1925-04-18</td>\n",
       "      <td>[Schwerte, Westhofen (Kreis Iserlohn), Schwert...</td>\n",
       "      <td>daß suric geklätt Frage arbei Unter Insicht , ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Schwerter Zeitung : Heimatblatt für die Stadt ...</td>\n",
       "      <td>1925-04-18</td>\n",
       "      <td>[Schwerte, Westhofen (Kreis Iserlohn), Schwert...</td>\n",
       "      <td>ilen spa sei sehr zulklassen or allem zegen di...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_title publication_date  \\\n",
       "1   Schwerter Zeitung : Heimatblatt für die Stadt ...       1925-04-18   \n",
       "2   Schwerter Zeitung : Heimatblatt für die Stadt ...       1925-04-18   \n",
       "3   Schwerter Zeitung : Heimatblatt für die Stadt ...       1925-04-18   \n",
       "10  Schwerter Zeitung : Heimatblatt für die Stadt ...       1925-04-18   \n",
       "11  Schwerter Zeitung : Heimatblatt für die Stadt ...       1925-04-18   \n",
       "\n",
       "                                place_of_distribution  \\\n",
       "1   [Schwerte, Westhofen (Kreis Iserlohn), Schwert...   \n",
       "2   [Schwerte, Westhofen (Kreis Iserlohn), Schwert...   \n",
       "3   [Schwerte, Westhofen (Kreis Iserlohn), Schwert...   \n",
       "10  [Schwerte, Westhofen (Kreis Iserlohn), Schwert...   \n",
       "11  [Schwerte, Westhofen (Kreis Iserlohn), Schwert...   \n",
       "\n",
       "                                    plainpagefulltext  pagenumber  \n",
       "1   besonders stark leidenden Magerkohlenzechen be...           2  \n",
       "2   sung von 8000 . r auf meine An e wäre völlig e...           3  \n",
       "3   * Im Interesse des ganzen Volkes liegt es , da...           4  \n",
       "10  daß suric geklätt Frage arbei Unter Insicht , ...          11  \n",
       "11  ilen spa sei sehr zulklassen or allem zegen di...          12  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/full_text_dfs_krieg/newspapers_ger_1925_part_1_krieg\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e540452",
   "metadata": {},
   "source": [
    "Doing sentiment analysis on the first 8 articles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5ace8dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list= [df.iloc[x, -2] for x in range(0,8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9107173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/rallypal/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "62361f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    # Initialize the VADER sentiment analyzer\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    sentiment_scores = sid.polarity_scores(text)\n",
    "    \n",
    "    # Determine sentiment based on compound score\n",
    "    if sentiment_scores['compound'] >= 0.05:\n",
    "        sentiment = \"Positive\"\n",
    "    elif sentiment_scores['compound'] <= -0.05:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return sentiment, sentiment_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ec3407e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n"
     ]
    }
   ],
   "source": [
    "for article in article_list:\n",
    "    \n",
    "    sentiment, sentiment_scores = analyze_sentiment(text)\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "    print(\"Sentiment Scores:\", sentiment_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe6c873a",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_news= [\n",
    "    'Das Geschäft brummt: Deutsche Unternehmen erzielen Rekordgewinne Für Deutschlands große DAX-Unternehmen lief es trotz trotz der hohen Energiepreise und der hohen Inflation ziemlich gut. Im dritten Quartal von Juli bis Ende September legten die Umsätze der 40 Top-DAX-Konzerne um 23 Prozent zu, die Gewinne sogar um 28 Prozent, wie die Unternehmensberatung EY mitteilte. Gewinnstärkste Unternehmen waren Mercedes, Volkswagen und Siemens.Wer mit einem Einbruch des Geschäfts gerechnet hatte, sieht sich getäuscht: Bei der Mehrzahl der DAX-Unternehmen steigen Umsatz und Gewinn, das Geschäft brummt, erklärte Henrik Ahlers von EY. Das Beratungsunternehmen rechnet sogar damit, dass 2022 in Summe ein Rekordjahr wird.',\n",
    "    'Normalerweise dauern Großbauprojekte deutlich länger als ursprünglich geplant. Man denke an die Elbphilharmonie in Hamburg, die 2010 fertig gestellt werden sollte und wo schließlich erst 2016 die Bauarbeiten beendet waren. Doch es geht auch anders: Nach nur knapp zehn Monaten Planungs- und Bauzeit ist kurz vor Weihnachten im norddeutschen Wilhelmshaven der erste schwimmende Importterminal für Flüssigerdgas (LNG) eröffnet worden - dank gestraffter Planungsverfahren. Das ist jetzt das neue Deutschland-Tempo, mit dem wir Infrastruktur voranbringen und es soll Vorbild sein, nicht nur für diese Anlage, sondern noch für viele, viele andere, sagte Bundeskanzler Olaf Scholz bei der Einweihungsfeier. Das schwimmende Terminal vor der niedersächsischen Nordseeküste soll dazu beitragen, die Lücke bei der Gasversorgung in Deutschland zu schließen, die durch ausbleibende Lieferungen aus Russland entstanden ist.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c8d9f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n",
      "Sentiment: Negative\n",
      "Sentiment Scores: {'neg': 0.151, 'neu': 0.845, 'pos': 0.004, 'compound': -0.9998}\n"
     ]
    }
   ],
   "source": [
    "for article in good_news:\n",
    "    \n",
    "    sentiment, sentiment_scores = analyze_sentiment(text)\n",
    "    print(\"Sentiment:\", sentiment)\n",
    "    print(\"Sentiment Scores:\", sentiment_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25d551",
   "metadata": {},
   "source": [
    "<font color=red>The sentiment analysis code is not working well. Even the sentiment in the good news is negative! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0999c6",
   "metadata": {},
   "source": [
    "Doing topic modeling on the first 8 articles in the df: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e0e94e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/rallypal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/rallypal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('german'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_text)\n",
    "\n",
    "def topic_modeling(texts, num_topics=5):\n",
    "    # Tokenize and remove stop words from the texts\n",
    "    tokenized_texts = [gensim.utils.simple_preprocess(remove_stopwords(text)) for text in texts]\n",
    "    \n",
    "    # Create a dictionary mapping from words to their integer ids\n",
    "    dictionary = corpora.Dictionary(tokenized_texts)\n",
    "    \n",
    "    # Create a corpus from the tokenized texts (a list of lists of words)\n",
    "    corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "    \n",
    "    # Build the LDA model\n",
    "    lda_model = gensim.models.LdaModel(corpus=corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state=42,\n",
    "                                       passes=10)\n",
    "    \n",
    "    return lda_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6115e8",
   "metadata": {},
   "source": [
    "The result of the following code snippet shows that topic modeling prior to thorough text cleaning is senseless: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a93abf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.008*\"uhr\" + 0.005*\"wurde\" + 0.004*\"sei\" + 0.004*\"stadtv\" + 0.004*\"april\" + 0.003*\"space\" + 0.002*\"worden\" + 0.002*\"ge\" + 0.002*\"stadt\" + 0.002*\"jahre\"\n",
      "Topic 1: 0.008*\"uhr\" + 0.003*\"münster\" + 0.003*\"ge\" + 0.003*\"space\" + 0.003*\"schwerte\" + 0.003*\"nr\" + 0.003*\"abends\" + 0.002*\"sonntag\" + 0.002*\"dortmund\" + 0.002*\"mannschaft\"\n",
      "Topic 2: 0.000*\"marquis\" + 0.000*\"heimat\" + 0.000*\"april\" + 0.000*\"wurde\" + 0.000*\"space\" + 0.000*\"sei\" + 0.000*\"uhr\" + 0.000*\"ge\" + 0.000*\"barmat\" + 0.000*\"lady\"\n",
      "Topic 3: 0.015*\"barmat\" + 0.008*\"abg\" + 0.007*\"april\" + 0.004*\"worden\" + 0.004*\"wurde\" + 0.004*\"sei\" + 0.004*\"beziehungen\" + 0.003*\"tel\" + 0.003*\"schulz\" + 0.003*\"be\"\n",
      "Topic 4: 0.004*\"herzog\" + 0.004*\"zipfelmeyer\" + 0.003*\"hasen\" + 0.003*\"uhr\" + 0.003*\"ernst\" + 0.003*\"hoheit\" + 0.002*\"schon\" + 0.002*\"sonntag\" + 0.002*\"pfg\" + 0.002*\"ten\"\n"
     ]
    }
   ],
   "source": [
    "lda_model = topic_modeling(article_list)\n",
    "\n",
    "# Print the topics and their corresponding words\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic {}: {}\".format(idx, topic))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
