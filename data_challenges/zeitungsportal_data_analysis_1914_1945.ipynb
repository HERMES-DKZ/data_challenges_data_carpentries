{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aebfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy import geocoders\n",
    "import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5621065",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# loading pickled dataframes, making new dataframes out of them and saving them to the local drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d90985",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def pickled_df_loader (year, columns):\n",
    "#     \"\"\"\n",
    "#     loads dataframes from the pickled dfs. \n",
    "#     \"\"\"\n",
    "#     df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_1\")[columns]\n",
    "#     df= pd.concat ([df, pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_2\")[columns]]\\\n",
    "#                    , axis=0)\n",
    "#     df= pd.concat ([df, pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_3\")[columns]]\\\n",
    "#                    , axis=0)\n",
    "    \n",
    "#     df['publication_date']= df['publication_date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "#     df['publication_date'] = pd.to_datetime(df['publication_date'], format='%Y-%m-%d')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a91b57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def challenge_df_generator (begin, end):\n",
    "#     \"\"\"\n",
    "#     loads all pickled dataframes belonging to the years in the year range and returns their concatenation as a single df. \n",
    "#     \"\"\" \n",
    "    \n",
    "#     columns= ['paper_title', 'publication_date', 'place_of_distribution']\n",
    "    \n",
    "#     df= pickled_df_loader (begin, columns)\n",
    "    \n",
    "#     for year in range (begin+1, end+1):\n",
    "#         df= pd.concat([df, pickled_df_loader(year, columns)], axis=0)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132237da",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_1915_part_1\")\n",
    "# df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe922d6c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyzing the dataframe containing all the data between 1914 and 1945:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c4a1ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# newspapers_1914_1945_df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_no_article_14_45\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e3fb4c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b06ca92",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a636124",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b85348",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's see what we can do with the lists of the cities: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301e8e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting all places of distribution and storing them in a pickled python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352e0bd5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# unique_cities=[]\n",
    "# for city in newspapers_1914_1945_df['place_of_distribution']:\n",
    "#     if city not in unique_cities: \n",
    "#         unique_cities.append(city)\n",
    "        \n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/unique_cities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(unique_cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84dc992",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# city_list= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/unique_cities.pkl\")\n",
    "# city_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa8690",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting names of single cities from the pickled city list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9722b07",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single_city_list= []\n",
    "\n",
    "# for place in city_list:\n",
    "#     if isinstance (place, list):\n",
    "#         for i in range(0, len(place)):\n",
    "#             if place[i] not in single_city_list:\n",
    "#                 single_city_list.append(place[i])\n",
    "#     else: \n",
    "#         if place not in city_list:\n",
    "#             city_list.append(place)\n",
    "\n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(single_city_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8210913",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cities_1914_1945= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\")\n",
    "# cities_1914_1945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300f1c8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pinning the cities from a city list on the world map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88ae075",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def map_maker(city_list):\n",
    "# #plots the cities in the city list on a map.\n",
    "    \n",
    "#     city_dict= {}\n",
    "#     gn= geocoders.GeoNames(username=\"golisf\")\n",
    "    \n",
    "#     for city in city_list:\n",
    "#         if not pd.isna(city):\n",
    "#             city_dict[city]= gn.geocode(city)\n",
    "            \n",
    "#     # Create a map centered at a location, you can adjust the coordinates and zoom level as needed\n",
    "#     map_center= [51.1657, 10.4515]  # Germany's approximate center\n",
    "#     my_map= folium.Map(location=map_center, zoom_start=6)\n",
    " \n",
    "#     # Add markers for each city\n",
    "#     for city in list(city_dict.keys()):\n",
    "#         folium.Marker(location=city_dict[city], popup=city).add_to(my_map)\n",
    "        \n",
    "#     return my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0357122",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cities_1914_1945= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\")\n",
    "# map_maker(cities_1914_1945)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea123aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's see what we can do with the paper titles and publication years: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c825bc8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting the titles of available newspapers and storing them in a pickled python list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b429bf75",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# unique_papers=[]\n",
    "# for paper in newspapers_1914_1945_df['paper_title']:\n",
    "#     if paper not in unique_papers: \n",
    "#         unique_papers.append(paper)\n",
    "        \n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/unique_papers.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(unique_papers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38498da8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_papers= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/unique_papers.pkl\")\n",
    "# all_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89289b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a df with the following columns: \n",
    "\n",
    "- name of newspaper\n",
    "- cities of distribution\n",
    "- date begin\n",
    "- date end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6b8e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df = newspapers_1914_1945_df.groupby('paper_title').agg(\n",
    "#     publication_begin=('publication_date', 'min'),\n",
    "#     publication_end=('publication_date', 'max'),\n",
    "#     place_of_distribution=('place_of_distribution', 'first')\n",
    "# ).reset_index()\n",
    "\n",
    "# paper_publication_df.to_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52689",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Running the above code, I realized that the same newspaper sometimes appears multiple times in the title, each time with minor changes (eg. issues and numbers are added to the title). So I optimized the code as below. In the below code, the newspaper title is tokenized and the the first three tokens of the title are added to the df as a separate column. Then the newspapers are grouped by the first three tokens and the newspaper_title is replaced by the shortest of newspaper title that shares the first three tokens. \n",
    "\n",
    "Question: Could it be that the different versions of the same newspaper have been distributed in different cities? \n",
    "\n",
    "- Perhaps the cities should not play an important role in the challegens. \n",
    "- Perhaps I have to write a more sophisticated code that combines all distribution places of all the versions of a single newspaper into a longer list and adds this list to the final paper_publication_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9199cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Function to extract first three tokens from a string\n",
    "# def extract_tokens(title):\n",
    "#     tokens = title.split()\n",
    "#     return ' '.join(tokens[:3])\n",
    "\n",
    "# # Apply the function to create a new column for the first three tokens\n",
    "# newspapers_1914_1945_df['first_three_tokens'] = newspapers_1914_1945_df['paper_title'].apply(extract_tokens)\n",
    "\n",
    "# # Group by the first three tokens and aggregate to get the shortest title\n",
    "# paper_publication_df = newspapers_1914_1945_df.groupby('first_three_tokens').agg(\n",
    "#     paper_title=('paper_title', lambda x: min(x, key=len)),\n",
    "#     publication_begin=('publication_date', 'min'),\n",
    "#     publication_end=('publication_date', 'max'),\n",
    "#     place_of_distribution=('place_of_distribution', 'first')\n",
    "# ).reset_index()\n",
    "\n",
    "# # Drop the temporary column\n",
    "# paper_publication_df.drop(columns='first_three_tokens', inplace=True)\n",
    "\n",
    "# paper_publication_df.to_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837a6f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see which newspapers were published between which years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9f7353",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")\n",
    "# paper_publication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f5e8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61931e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how many \"Fachzeitschriften\" there are among the newspapers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b26ee23",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fach_papers= [paper for paper in paper_publication_df['paper_title'] if 'fach' in paper.lower()]\n",
    "# paper_publication_df[paper_publication_df['paper_title'].isin(fach_papers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586ead9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see for how long each newspaper has been published after 1914:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef812f40",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# fig, ax= plt.subplots(figsize=(30, 15))\n",
    "# ax.set_title('Publication span of each newspaper.' ,fontsize=25)\n",
    "\n",
    "# for i, row in paper_publication_df.iterrows():\n",
    "#     bar_length= (row['publication_end'].year - row['publication_begin'].year)\n",
    "    \n",
    "#     if bar_length>0:\n",
    "#         ax.barh(row['paper_title'][0:50], bar_length, color='g', left=row['publication_begin'].year)\n",
    "#     else:\n",
    "#         #the short red bars represent newspapers that were only published within a year. \n",
    "#         ax.barh(row['paper_title'][0:50], 0.1, color='r', left=row['publication_begin'].year)\n",
    "    \n",
    "# ax.set_xlim (1913, 1946)\n",
    "# ax.tick_params(axis=\"x\", bottom=True, top=True, labelbottom=True, labeltop=True)\n",
    "# #ax.xaxis.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "# plt.xticks(ticks=np.arange(1914, 1946), rotation=45)\n",
    "# plt.rcParams['xtick.labelsize'] = 15\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca5612",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "1. Interestingly, the publication of a considerable amount of papers seems to have stopped in 1933.\n",
    "2. Despite applying the function that extracted the first three tokens from the newspaper titles, there are still instances of the same newspaper with two different titles, one shorter and one longer. More cleaning has to be done on the titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de810239",
   "metadata": {},
   "source": [
    "# Doing PoS tagging and NER on example articles, using spaCy and network analysis using NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fac4f43",
   "metadata": {},
   "source": [
    "## spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e53cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661c312e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_1931_part_1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947f526a",
   "metadata": {},
   "source": [
    "Are there any \"Fachzeitschriften\" in df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b43147",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fach_papers= [paper for paper in df['paper_title'] if 'fach' in paper.lower()]\n",
    "# df[df['paper_title'].isin(fach_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceac25f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_df=df.sample(3)\n",
    "random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180d67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1= random_df.iloc[0,9]\n",
    "text_2= random_df.iloc[1,9]\n",
    "text_3= random_df.iloc[2,9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8231adb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#! python -m spacy download de_core_news_sm\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "doc_1 = nlp(text_1)\n",
    "sentences_1 = list(doc_1.sents)\n",
    "\n",
    "doc_2 = nlp(text_2)\n",
    "sentences_2 = list(doc_2.sents)\n",
    "\n",
    "doc_3 = nlp(text_3)\n",
    "sentences_3 = list(doc_3.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd70cb8",
   "metadata": {},
   "source": [
    "PoS (Part of Speech) tagging in the first sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7a497",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for token in sentences_1[0]:\n",
    "    print('{}: {}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae29efd3",
   "metadata": {},
   "source": [
    "A function that extracts all nouns from all sentences within a given text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0983972",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    entities=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_entities = []\n",
    "        for token in sentence:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                sentence_entities.append(token.text)\n",
    "    \n",
    "        if len(sentence_entities)>0:\n",
    "            entities.append(sentence_entities)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6f262",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_1= extract_nouns(text_1)\n",
    "nouns_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556febec",
   "metadata": {},
   "source": [
    "### Coverting noun lists into network data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d0e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_data(nouns):\n",
    "    final_sources = []\n",
    "    final_targets = []\n",
    "    for row in nouns:\n",
    "        source = row[0]\n",
    "        targets = row[1:]\n",
    "        for target in targets:\n",
    "            final_sources.append(source)\n",
    "            final_targets.append(target)\n",
    "    df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ca945",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_df_1= get_network_data(nouns_1).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea78ae63",
   "metadata": {},
   "source": [
    "### Converting network data into visualized networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5319c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_1 = nx.from_pandas_edgelist(network_df_1)\n",
    "nx.info(G_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a03116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5, edge_length=100):\n",
    "    \n",
    "    import numpy as np\n",
    "    from IPython.display import SVG\n",
    "    from sknetwork.visualization import svg_graph\n",
    "    from sknetwork.data import Bunch\n",
    "    from sknetwork.ranking import PageRank\n",
    "    \n",
    "#     # Compute force-directed layout\n",
    "#     pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "    adjacency = nx.to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight='weight', format='csr')\n",
    "    names = np.array(list(G.nodes()))\n",
    "    graph = Bunch()\n",
    "    graph.adjacency = adjacency\n",
    "    graph.names = np.array(names)\n",
    "    pagerank = PageRank()\n",
    "    scores = pagerank.fit_transform(adjacency)\n",
    "    if show_names:\n",
    "        image = svg_graph(graph.adjacency, font_size=font_size,\n",
    "                          node_size=node_size, names=graph.names, width=700, height=500,\n",
    "                          scores=scores, edge_width=edge_width)\n",
    "    else:\n",
    "        image = svg_graph(graph.adjacency, node_size=node_size,\n",
    "                          width=700, height=500, scores = scores, edge_width=edge_width, edge_length=edge_length)\n",
    "    \n",
    "    return SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa609d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "draw_graph(G_1, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be918c63",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns_2= extract_nouns(text_2)\n",
    "network_df_2= get_network_data(nouns_2).head(40)\n",
    "G_2 = nx.from_pandas_edgelist(network_df_2)\n",
    "draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c7666",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nouns_3= extract_nouns(text_3)\n",
    "network_df_3= get_network_data(nouns_3).head(40)\n",
    "G_3 = nx.from_pandas_edgelist(network_df_3)\n",
    "draw_graph(G_3, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55dd954",
   "metadata": {},
   "source": [
    "<font color=red>RESULT:</font> Finding nouns in the newspaper data is useless, because there are lots of them and some of them are not relevant. Except we search for certain nouns such as \"Nazis\" or \"Nationalsozialisten\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c0c0e",
   "metadata": {},
   "source": [
    "NER (Named Entity Recognition) with spaCy in the first article: ðŸ‘ŽðŸ½"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163178bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ent in doc_2.ents:\n",
    "#     print('{}: {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e4b1e9",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0b774e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d42424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "sentences_1 = sent_tokenize(text_1)\n",
    "sentences_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3c528",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = sentences_1[3]\n",
    "\n",
    "from nltk.tokenize import casual_tokenize\n",
    "tokens = casual_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd25c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e22fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "\n",
    "def extract_entities(sentence):\n",
    "    \"\"\"\n",
    "    extracts entities from single sentences.\n",
    "    \"\"\"\n",
    "    \n",
    "    entities = []\n",
    "    tokens = casual_tokenize(sentence)\n",
    "    \n",
    "    for row in nltk.pos_tag(tokens):\n",
    "        token = row[0]\n",
    "        tag = row[1]\n",
    "        if tag == 'NNP':\n",
    "            for p in punctuation:\n",
    "                if p in token:\n",
    "                    cutoff = token.index(p)\n",
    "                    token = token[:cutoff]\n",
    "            if len(token) > 1:\n",
    "                entities.append(token)\n",
    "            \n",
    "    if len(entities) > 0:\n",
    "        return entities\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9751218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities = [extract_entities(sentence) for sentence in sentences_1]\n",
    "# len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05926509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame({'sentence':sentences_1, 'entities':entities})\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ed0a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna()\n",
    "# df = df[df['entities'].apply(len)>1]\n",
    "# entities = df['entities'].to_list()\n",
    "# len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf007e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_text_entities(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    entities = [extract_entities(sentence) for sentence in sentences]\n",
    "    df = pd.DataFrame({'sentence':sentences, 'entities':entities})\n",
    "    df = df.dropna()\n",
    "    df = df[df['entities'].apply(len) > 1]\n",
    "    entities = df['entities'].to_list()\n",
    "    \n",
    "    german_stop_words = stopwords.words('german')\n",
    "    \n",
    "    for list in entities: \n",
    "        for item in list:\n",
    "            if item.lower() in german_stop_words:\n",
    "                list.remove(item)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02345de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_entities (text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a29a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_entities (text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_text_entities (text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c66117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_df_1= get_network_data(get_text_entities (text_1))\n",
    "network_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e44212e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "G_1 = nx.from_pandas_edgelist(network_df_1)\n",
    "draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930679b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_df_2= get_network_data(nouns_2).head(40)\n",
    "G_2 = nx.from_pandas_edgelist(network_df_2)\n",
    "draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6787cb17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "network_df_3= get_network_data(nouns_3).head(40)\n",
    "G_3 = nx.from_pandas_edgelist(network_df_3)\n",
    "draw_graph(G_3, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71001ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_1= extract_nouns(text_1)\n",
    "# nouns_2= extract_nouns(text_2)\n",
    "# nouns_3= extract_nouns(text_3)\n",
    "\n",
    "len(nouns_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8697b9ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(get_text_entities (text_1))\n",
    "# get_text_entities (text_2)\n",
    "# get_text_entities (text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416bb95",
   "metadata": {},
   "source": [
    "## result: spaCy performs much better than NLTK in extracting nouns and needs less cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba5e060",
   "metadata": {},
   "source": [
    "# Removing white spaces that split words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677f9a87",
   "metadata": {},
   "source": [
    "Here is a list of all words in the German language: \n",
    "\n",
    "https://gist.github.com/MarvinJWendt/2f4f4154b8ae218600eb091a5706b5f4\n",
    "\n",
    "To remove the extra white spaces, we can check if the words exist in this list. If not, we can remove one white space after them and check if the resulting word is now available in the list. If yes, then the resulting word replaces the previous two words plus the space between them. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
