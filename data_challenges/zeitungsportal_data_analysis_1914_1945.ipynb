{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3aebfe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from geopy import geocoders\n",
    "# import folium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5621065",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# loading pickled dataframes, making new dataframes out of them and saving them to the local drive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d90985",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def pickled_df_loader (year, columns):\n",
    "#     \"\"\"\n",
    "#     loads dataframes from the pickled dfs. \n",
    "#     \"\"\"\n",
    "#     df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_1\")[columns]\n",
    "#     df= pd.concat ([df, pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_2\")[columns]]\\\n",
    "#                    , axis=0)\n",
    "#     df= pd.concat ([df, pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_3\")[columns]]\\\n",
    "#                    , axis=0)\n",
    "    \n",
    "#     df['publication_date']= df['publication_date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "#     df['publication_date'] = pd.to_datetime(df['publication_date'], format='%Y-%m-%d')\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a91b57",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def challenge_df_generator (begin, end):\n",
    "#     \"\"\"\n",
    "#     loads all pickled dataframes belonging to the years in the year range and returns their concatenation as a single df. \n",
    "#     \"\"\" \n",
    "    \n",
    "#     columns= ['paper_title', 'publication_date', 'place_of_distribution']\n",
    "    \n",
    "#     df= pickled_df_loader (begin, columns)\n",
    "    \n",
    "#     for year in range (begin+1, end+1):\n",
    "#         df= pd.concat([df, pickled_df_loader(year, columns)], axis=0)\n",
    "        \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe922d6c",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Analyzing the dataframe containing all the data between 1914 and 1945:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c4a1ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# newspapers_1914_1945_df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_no_article_14_45\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86e3fb4c",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b06ca92",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a636124",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#newspapers_1914_1945_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b85348",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's see what we can do with the lists of the cities: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e301e8e7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting all places of distribution and storing them in a pickled python list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352e0bd5",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# unique_cities=[]\n",
    "# for city in newspapers_1914_1945_df['place_of_distribution']:\n",
    "#     if city not in unique_cities: \n",
    "#         unique_cities.append(city)\n",
    "        \n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/unique_cities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(unique_cities, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84dc992",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# city_list= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/unique_cities.pkl\")\n",
    "# city_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa8690",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting names of single cities from the pickled city list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9722b07",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# single_city_list= []\n",
    "\n",
    "# for place in city_list:\n",
    "#     if isinstance (place, list):\n",
    "#         for i in range(0, len(place)):\n",
    "#             if place[i] not in single_city_list:\n",
    "#                 single_city_list.append(place[i])\n",
    "#     else: \n",
    "#         if place not in city_list:\n",
    "#             city_list.append(place)\n",
    "\n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(single_city_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8210913",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cities_1914_1945= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\")\n",
    "# cities_1914_1945"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4300f1c8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Pinning the cities from a city list on the world map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a88ae075",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def map_maker(city_list):\n",
    "# #plots the cities in the city list on a map.\n",
    "    \n",
    "#     city_dict= {}\n",
    "#     gn= geocoders.GeoNames(username=\"golisf\")\n",
    "    \n",
    "#     for city in city_list:\n",
    "#         if not pd.isna(city):\n",
    "#             city_dict[city]= gn.geocode(city)\n",
    "            \n",
    "#     # Create a map centered at a location, you can adjust the coordinates and zoom level as needed\n",
    "#     map_center= [51.1657, 10.4515]  # Germany's approximate center\n",
    "#     my_map= folium.Map(location=map_center, zoom_start=6)\n",
    " \n",
    "#     # Add markers for each city\n",
    "#     for city in list(city_dict.keys()):\n",
    "#         folium.Marker(location=city_dict[city], popup=city).add_to(my_map)\n",
    "        \n",
    "#     return my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0357122",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# cities_1914_1945= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/single_cities.pkl\")\n",
    "# map_maker(cities_1914_1945)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea123aa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Let's see what we can do with the paper titles and publication years: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c825bc8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Extracting the titles of available newspapers and storing them in a pickled python list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b429bf75",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# unique_papers=[]\n",
    "# for paper in newspapers_1914_1945_df['paper_title']:\n",
    "#     if paper not in unique_papers: \n",
    "#         unique_papers.append(paper)\n",
    "        \n",
    "# import pickle\n",
    "# with open(f\"./data_deutsches_zeitungsportal_1914_1945/unique_papers.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(unique_papers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38498da8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# all_papers= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/unique_papers.pkl\")\n",
    "# all_papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89289b6",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a df with the following columns: \n",
    "\n",
    "- name of newspaper\n",
    "- cities of distribution\n",
    "- date begin\n",
    "- date end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaa6b8e4",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df = newspapers_1914_1945_df.groupby('paper_title').agg(\n",
    "#     publication_begin=('publication_date', 'min'),\n",
    "#     publication_end=('publication_date', 'max'),\n",
    "#     place_of_distribution=('place_of_distribution', 'first')\n",
    "# ).reset_index()\n",
    "\n",
    "# paper_publication_df.to_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f52689",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Running the above code, I realized that the same newspaper sometimes appears multiple times in the title, each time with minor changes (eg. issues and numbers are added to the title). So I optimized the code as below. In the below code, the newspaper title is tokenized and the the first three tokens of the title are added to the df as a separate column. Then the newspapers are grouped by the first three tokens and the newspaper_title is replaced by the shortest of newspaper title that shares the first three tokens. \n",
    "\n",
    "Question: Could it be that the different versions of the same newspaper have been distributed in different cities? \n",
    "\n",
    "- Perhaps the cities should not play an important role in the challegens. \n",
    "- Perhaps I have to write a more sophisticated code that combines all distribution places of all the versions of a single newspaper into a longer list and adds this list to the final paper_publication_df. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b9199cb",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # Function to extract first two tokens from a string\n",
    "# def extract_tokens(title):\n",
    "#     tokens = title.split()\n",
    "#     return ' '.join(tokens[:2])\n",
    "\n",
    "# # Apply the function to create a new column for the first three tokens\n",
    "# newspapers_1914_1945_df['first_two_tokens'] = newspapers_1914_1945_df['paper_title'].apply(extract_tokens)\n",
    "\n",
    "# # Group by the first three tokens and aggregate to get the shortest title\n",
    "# paper_publication_df = newspapers_1914_1945_df.groupby('first_two_tokens').agg(\n",
    "#     paper_title=('paper_title', lambda x: min(x, key=len)),\n",
    "#     publication_begin=('publication_date', 'min'),\n",
    "#     publication_end=('publication_date', 'max'),\n",
    "#     place_of_distribution=('place_of_distribution', 'first')\n",
    "# ).reset_index()\n",
    "\n",
    "# # Drop the temporary column\n",
    "# paper_publication_df.drop(columns='first_two_tokens', inplace=True)\n",
    "\n",
    "# paper_publication_df.to_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d837a6f2",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see which newspapers were published between which years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f9f7353",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/paper_publication_df\")\n",
    "# paper_publication_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca8f5e8f",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# paper_publication_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f61931e",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see how many \"Fachzeitschriften\" there are among the newspapers: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b26ee23",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# fach_papers= [paper for paper in paper_publication_df['paper_title'] if 'fach' in paper.lower()]\n",
    "# paper_publication_df[paper_publication_df['paper_title'].isin(fach_papers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5586ead9",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's see for how long each newspaper has been published after 1914:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ef812f40",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "# fig, ax= plt.subplots(figsize=(30, 15))\n",
    "# ax.set_title('Publication span of each newspaper.' ,fontsize=25)\n",
    "\n",
    "# for i, row in paper_publication_df.iterrows():\n",
    "#     bar_length= (row['publication_end'].year - row['publication_begin'].year)\n",
    "    \n",
    "#     if bar_length>0:\n",
    "#         ax.barh(row['paper_title'][0:50], bar_length, color='g', left=row['publication_begin'].year)\n",
    "#     else:\n",
    "#         #the short red bars represent newspapers that were only published within a year. \n",
    "#         ax.barh(row['paper_title'][0:50], 0.1, color='r', left=row['publication_begin'].year)\n",
    "    \n",
    "# ax.set_xlim (1913, 1946)\n",
    "# ax.tick_params(axis=\"x\", bottom=True, top=True, labelbottom=True, labeltop=True)\n",
    "# #ax.xaxis.grid(color='gray', linestyle='-', linewidth=0.5)\n",
    "# plt.xticks(ticks=np.arange(1914, 1946), rotation=45)\n",
    "# plt.rcParams['xtick.labelsize'] = 15\n",
    "\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ca5612",
   "metadata": {
    "hidden": true
   },
   "source": [
    "**Notes:**\n",
    "\n",
    "1. Interestingly, the publication of a considerable amount of papers seems to have stopped in 1933.\n",
    "2. Despite applying the function that extracted the first three tokens from the newspaper titles, there are still instances of the same newspaper with two different titles, one shorter and one longer. More cleaning has to be done on the titles. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de810239",
   "metadata": {},
   "source": [
    "# Doing PoS tagging and NER on example articles, using spaCy and network analysis using NetworkX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a768f6",
   "metadata": {},
   "source": [
    "## spaCy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c51a052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df43b242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#df= pd.read_pickle(\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_1931_part_1\")\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5342323",
   "metadata": {},
   "source": [
    "Are there any \"Fachzeitschriften\" in df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08245821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fach_papers= [paper for paper in df['paper_title'] if 'fach' in paper.lower()]\n",
    "# df[df['paper_title'].isin(fach_papers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b6aa558",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# random_df=df.sample(3)\n",
    "# random_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "422e03eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6df34190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_1= random_df.iloc[0,9]\n",
    "# text_2= random_df.iloc[1,9]\n",
    "# text_3= random_df.iloc[2,9]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152c3e65",
   "metadata": {},
   "source": [
    "Here is the link to the page in which the text_1 appears: https://www.deutsche-digitale-bibliothek.de/newspaper/item/B57SF7JW24X55ZNNO5AENGAPSBQLHFYC?tx_dlf[highlight_word]=Neue%2BMannheimer%2BZeitung&issuepage=8&query=Neue+Mannheimer+Zeitung&fromDay=2&fromMonth=3&fromYear=1931&toDay=2&toMonth=3&toYear=1931&hit=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8231adb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #! python -m spacy download de_core_news_sm\n",
    "# nlp = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "# doc_1 = nlp(text_1)\n",
    "# sentences_1 = list(doc_1.sents)\n",
    "\n",
    "# doc_2 = nlp(text_2)\n",
    "# sentences_2 = list(doc_2.sents)\n",
    "\n",
    "# doc_3 = nlp(text_3)\n",
    "# sentences_3 = list(doc_3.sents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7ad4a0",
   "metadata": {},
   "source": [
    "PoS (Part of Speech) tagging in the first sentence: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40992de8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for token in sentences_1[0]:\n",
    "#     print('{}: {}'.format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce192872",
   "metadata": {},
   "source": [
    "A function that extracts all nouns from all sentences within a given text: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c52b07d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_nouns(text):\n",
    "    \n",
    "#     doc = nlp(text)\n",
    "#     sentences = list(doc.sents)\n",
    "#     entities=[]\n",
    "    \n",
    "#     for sentence in sentences:\n",
    "#         sentence_entities = []\n",
    "#         for token in sentence:\n",
    "#             if token.pos_ == 'NOUN':\n",
    "#                 sentence_entities.append(token.text)\n",
    "    \n",
    "#         if len(sentence_entities)>0:\n",
    "#             entities.append(sentence_entities)\n",
    "    \n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fd746d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nouns_1= extract_nouns(text_1)\n",
    "# nouns_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda54cf0",
   "metadata": {},
   "source": [
    "### Coverting noun lists into network data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "883fc194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_network_data(nouns):\n",
    "#     final_sources = []\n",
    "#     final_targets = []\n",
    "#     for row in nouns:\n",
    "#         source = row[0]\n",
    "#         targets = row[1:]\n",
    "#         for target in targets:\n",
    "#             final_sources.append(source)\n",
    "#             final_targets.append(target)\n",
    "#     df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cee0927",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network_df_1= get_network_data(nouns_1).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fb4d3e",
   "metadata": {},
   "source": [
    "### Converting network data into visualized networks: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "96c26b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_1 = nx.from_pandas_edgelist(network_df_1)\n",
    "# nx.info(G_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b9b7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def draw_graph(G, show_names=False, node_size=1, font_size=10, edge_width=0.5, edge_length=100):\n",
    "    \n",
    "#     import numpy as np\n",
    "#     from IPython.display import SVG\n",
    "#     from sknetwork.visualization import svg_graph\n",
    "#     from sknetwork.data import Bunch\n",
    "#     from sknetwork.ranking import PageRank\n",
    "    \n",
    "# #     # Compute force-directed layout\n",
    "# #     pos = nx.spring_layout(G, seed=42)\n",
    "    \n",
    "#     adjacency = nx.to_scipy_sparse_matrix(G, nodelist=None, dtype=None, weight='weight', format='csr')\n",
    "#     names = np.array(list(G.nodes()))\n",
    "#     graph = Bunch()\n",
    "#     graph.adjacency = adjacency\n",
    "#     graph.names = np.array(names)\n",
    "#     pagerank = PageRank()\n",
    "#     scores = pagerank.fit_transform(adjacency)\n",
    "#     if show_names:\n",
    "#         image = svg_graph(graph.adjacency, font_size=font_size,\n",
    "#                           node_size=node_size, names=graph.names, width=700, height=500,\n",
    "#                           scores=scores, edge_width=edge_width)\n",
    "#     else:\n",
    "#         image = svg_graph(graph.adjacency, node_size=node_size,\n",
    "#                           width=700, height=500, scores = scores, edge_width=edge_width, edge_length=edge_length)\n",
    "    \n",
    "#     return SVG(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0e53ae89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# draw_graph(G_1, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc114aed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nouns_2= extract_nouns(text_2)\n",
    "# network_df_2= get_network_data(nouns_2).head(40)\n",
    "# G_2 = nx.from_pandas_edgelist(network_df_2)\n",
    "# draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0c6d1a0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nouns_3= extract_nouns(text_3)\n",
    "# network_df_3= get_network_data(nouns_3).head(40)\n",
    "# G_3 = nx.from_pandas_edgelist(network_df_3)\n",
    "# draw_graph(G_3, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95089936",
   "metadata": {},
   "source": [
    "<font color=red>RESULT:</font> Finding nouns in the newspaper data is useless, because there are lots of them and some of them are not relevant. Except we search for certain nouns such as \"Nazis\" or \"Nationalsozialisten\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da1920",
   "metadata": {},
   "source": [
    "NER (Named Entity Recognition) with spaCy in the first article: 👎🏽"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12778892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ent in doc_2.ents:\n",
    "#     print('{}: {}'.format(ent, ent.label_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3f5d8",
   "metadata": {},
   "source": [
    "## NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c8585f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e64855d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# sentences_1 = sent_tokenize(text_1)\n",
    "# sentences_1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2bcb1a73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sentence = sentences_1[3]\n",
    "\n",
    "# from nltk.tokenize import casual_tokenize\n",
    "# tokens = casual_tokenize(sentence)\n",
    "# tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "284bb7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "747ed936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from string import punctuation\n",
    "\n",
    "# def extract_entities(sentence):\n",
    "#     \"\"\"\n",
    "#     extracts entities from single sentences.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     entities = []\n",
    "#     tokens = casual_tokenize(sentence)\n",
    "    \n",
    "#     for row in nltk.pos_tag(tokens):\n",
    "#         token = row[0]\n",
    "#         tag = row[1]\n",
    "#         if tag == 'NNP':\n",
    "#             for p in punctuation:\n",
    "#                 if p in token:\n",
    "#                     cutoff = token.index(p)\n",
    "#                     token = token[:cutoff]\n",
    "#             if len(token) > 1:\n",
    "#                 entities.append(token)\n",
    "            \n",
    "#     if len(entities) > 0:\n",
    "#         return entities\n",
    "#     else:\n",
    "#         return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c2fc98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# entities = [extract_entities(sentence) for sentence in sentences_1]\n",
    "# len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11f0c385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df = pd.DataFrame({'sentence':sentences_1, 'entities':entities})\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9a472dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna()\n",
    "# df = df[df['entities'].apply(len)>1]\n",
    "# entities = df['entities'].to_list()\n",
    "# len(entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "97537231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# def get_text_entities(text):\n",
    "    \n",
    "#     sentences = sent_tokenize(text)\n",
    "#     entities = [extract_entities(sentence) for sentence in sentences]\n",
    "#     df = pd.DataFrame({'sentence':sentences, 'entities':entities})\n",
    "#     df = df.dropna()\n",
    "#     df = df[df['entities'].apply(len) > 1]\n",
    "#     entities = df['entities'].to_list()\n",
    "    \n",
    "#     german_stop_words = stopwords.words('german')\n",
    "    \n",
    "#     for list in entities: \n",
    "#         for item in list:\n",
    "#             if item.lower() in german_stop_words:\n",
    "#                 list.remove(item)\n",
    "    \n",
    "#     return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "56912e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text_entities (text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a30e28d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text_entities (text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e957f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_text_entities (text_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba2a529b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_df_1= get_network_data(get_text_entities (text_1))\n",
    "# network_df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "49adae8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# G_1 = nx.from_pandas_edgelist(network_df_1)\n",
    "# draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2d0be36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network_df_2= get_network_data(nouns_2).head(40)\n",
    "# G_2 = nx.from_pandas_edgelist(network_df_2)\n",
    "# draw_graph(G_2, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "51378547",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# network_df_3= get_network_data(nouns_3).head(40)\n",
    "# G_3 = nx.from_pandas_edgelist(network_df_3)\n",
    "# draw_graph(G_3, edge_width=0.2, node_size=3, show_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57c0015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nouns_1= extract_nouns(text_1)\n",
    "# nouns_2= extract_nouns(text_2)\n",
    "# nouns_3= extract_nouns(text_3)\n",
    "\n",
    "#len(nouns_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088bfa13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# len(get_text_entities (text_1))\n",
    "# get_text_entities (text_2)\n",
    "# get_text_entities (text_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5cf8f2",
   "metadata": {},
   "source": [
    "## RESULT: spaCy performs much better than NLTK in extracting nouns and needs less cleaning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bf38c2",
   "metadata": {},
   "source": [
    "# Exploring the possibility of a challenge: Looking for War!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4eead7",
   "metadata": {},
   "source": [
    "Extracting only dataframes containing a certain list of words in their full text and saving them to pickled dataframes on the local drive again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12871b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f42d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickled_df_loader (year, part, columns, words):\n",
    "    \"\"\"\n",
    "    loads all three dataframes from the pickled dfs for each year and concatenates them. \n",
    "    \"\"\"\n",
    "    df= pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_{part}\")[columns]\n",
    "    \n",
    "#     for index, row in df.iterrows():\n",
    "#         if not any(word in row['plainpagefulltext'].lower() for word in words):\n",
    "#             df.drop(index, inplace=True)\n",
    "    \n",
    "    # Convert 'plainpagefulltext' to lowercase and check for presence of words using vectorized operations\n",
    "    # | (pipe) is a regular expression standing for \"or\".\n",
    "    mask = df['plainpagefulltext'].str.lower().str.contains('|'.join(words))\n",
    "    df = df[mask]\n",
    "    \n",
    "    df['publication_date']= df['publication_date'].apply(lambda x: x.strftime(\"%Y-%m-%d\"))\n",
    "    df['publication_date'] = pd.to_datetime(df['publication_date'], format='%Y-%m-%d')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07dd4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns= ['paper_title', 'publication_date', 'place_of_distribution', 'plainpagefulltext', 'pagenumber']\n",
    "words=['krieg']\n",
    "\n",
    "for year in range(1914, 1946):\n",
    "    for part in range(1,4):\n",
    "        df= pickled_df_loader (year, part, columns, words)\n",
    "        df.to_pickle (f\"./data_deutsches_zeitungsportal_1914_1945/newspapers_ger_{year}_part_{part}_krieg\")\n",
    "        print(f\"task for {year} part {part} complete. length= {len(df)}\")\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0af5a11",
   "metadata": {},
   "source": [
    "Now let us load one of the dataframes with the word \"krieg\" in all of its full texts and see what a network analysis of the word would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83c2d6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b940e942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_title</th>\n",
       "      <th>publication_date</th>\n",
       "      <th>place_of_distribution</th>\n",
       "      <th>plainpagefulltext</th>\n",
       "      <th>pagenumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Der Volksfreund : Tageszeitung für das werktät...</td>\n",
       "      <td>1914-03-04</td>\n",
       "      <td>[Karlsruhe, Offenburg]</td>\n",
       "      <td>tio . 63 . Karlsruhe , Mittwoch den 4 * Marz 1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Der Volksfreund : Tageszeitung für das werktät...</td>\n",
       "      <td>1914-03-04</td>\n",
       "      <td>[Karlsruhe, Offenburg]</td>\n",
       "      <td>No . 53 . Mittwoch , den 4 . März 1914 . Veite...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Der Volksfreund : Tageszeitung für das werktät...</td>\n",
       "      <td>1914-03-04</td>\n",
       "      <td>[Karlsruhe, Offenburg]</td>\n",
       "      <td>Nr . 53 , Mittwoch , den 4 . März 1914 . Soebe...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Bergische Wacht. 1907-1941</td>\n",
       "      <td>1914-02-25</td>\n",
       "      <td>[Klüppelberg, Wipperfürth, Wipperfürth-Klüppel...</td>\n",
       "      <td>mit den Beilagen : Illustr . Sonntagsblatt und...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Bergische Wacht. 1907-1941</td>\n",
       "      <td>1914-02-25</td>\n",
       "      <td>[Klüppelberg, Wipperfürth, Wipperfürth-Klüppel...</td>\n",
       "      <td>sie das Trostlose ihrer Lage eingesehen , wenn...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          paper_title publication_date  \\\n",
       "4   Der Volksfreund : Tageszeitung für das werktät...       1914-03-04   \n",
       "9   Der Volksfreund : Tageszeitung für das werktät...       1914-03-04   \n",
       "11  Der Volksfreund : Tageszeitung für das werktät...       1914-03-04   \n",
       "12                         Bergische Wacht. 1907-1941       1914-02-25   \n",
       "13                         Bergische Wacht. 1907-1941       1914-02-25   \n",
       "\n",
       "                                place_of_distribution  \\\n",
       "4                              [Karlsruhe, Offenburg]   \n",
       "9                              [Karlsruhe, Offenburg]   \n",
       "11                             [Karlsruhe, Offenburg]   \n",
       "12  [Klüppelberg, Wipperfürth, Wipperfürth-Klüppel...   \n",
       "13  [Klüppelberg, Wipperfürth, Wipperfürth-Klüppel...   \n",
       "\n",
       "                                    plainpagefulltext  pagenumber  \n",
       "4   tio . 63 . Karlsruhe , Mittwoch den 4 * Marz 1...           1  \n",
       "9   No . 53 . Mittwoch , den 4 . März 1914 . Veite...           6  \n",
       "11  Nr . 53 , Mittwoch , den 4 . März 1914 . Soebe...           8  \n",
       "12  mit den Beilagen : Illustr . Sonntagsblatt und...           1  \n",
       "13  sie das Trostlose ihrer Lage eingesehen , wenn...           2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_pickle(f\"./data_deutsches_zeitungsportal_1914_1945/full_text_dfs_krieg/newspapers_ger_1914_part_1_krieg\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061d755c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Hamburger Fremdenblatt\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['paper_title'].mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3104cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1150 entries, 184 to 127179\n",
      "Data columns (total 5 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   paper_title            1150 non-null   object        \n",
      " 1   publication_date       1150 non-null   datetime64[ns]\n",
      " 2   place_of_distribution  1150 non-null   object        \n",
      " 3   plainpagefulltext      1150 non-null   object        \n",
      " 4   pagenumber             1150 non-null   int64         \n",
      "dtypes: datetime64[ns](1), int64(1), object(3)\n",
      "memory usage: 53.9+ KB\n"
     ]
    }
   ],
   "source": [
    "hamburger_df=df[df['paper_title']=='Hamburger Fremdenblatt']\n",
    "hamburger_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbed9d2b",
   "metadata": {},
   "source": [
    "The newspaper \"Hamburger Fremdblatt\" seems to have the highest amount of articles in this df. I want to see how this particular newspaper has been reporting about war in the first four months of 1914. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b15515e",
   "metadata": {},
   "source": [
    "First, let us tokenize sentences and words and filter out every instance of 'krieg':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "09a8983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences(text, word):\n",
    "    \"\"\"\n",
    "    extracts all sentences that contain the word \"krieg\".\n",
    "    \"\"\" \n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    sentences_with_word=[]\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if word in str(token).lower():\n",
    "                sentences_with_word.append(sentence)\n",
    "            \n",
    "    return sentences_with_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c2933a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=hamburger_df.iloc[25,-2]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a1b97f50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Tapperheten Kriegsseh.,\n",
       " Es ist den vereinten Bemühungen der Kriegsschiffe Acran und Vasa sowie des Bergungs dampfers Herakles nicht gelungen, das gestrandete Panzerschiff Tappvvheten abzuschleppen, trotzdem die kleineren Geschütze, Kohlen usw. herausgenom men sind.,\n",
       " Die beiden obigen Kriegs schiffe sind zurückgezogen worden Tasealusa D. Algier, 31. Janr.]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_sentences(text, 'krieg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ba76fd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_with_krieg= []\n",
    "\n",
    "for i in range (0,len(hamburger_df)):\n",
    "    sents_with_krieg.append(extract_sentences(hamburger_df.iloc[i,-2], 'krieg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70b13e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1150"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents_with_krieg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87430d99",
   "metadata": {},
   "source": [
    "<font color=red> \n",
    "    \n",
    "We are here. Since I have manually selected out the sentences containing the word 'krieg', and have not done this with spacy, spacy does not recognize them as a list of tokens and the following cell returns an error. \n",
    "\n",
    "Next thing to do here would be to:\n",
    "   \n",
    "1. solve this problem and get all the words from the list of sentences which contain 'krieg';\n",
    "2. create network data based on these words;\n",
    "3. visualize the networks\n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "607f975c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_nouns(sentence_list):\n",
    "    entities = []\n",
    "    \n",
    "    for sentence in sentence_list:\n",
    "        sentence_entities = []\n",
    "        for token in sentence:\n",
    "            if token.pos_ == 'NOUN':\n",
    "                sentence_entities.append(token.text)\n",
    "    \n",
    "        if len(sentence_entities) > 0:\n",
    "            entities.append(sentence_entities)\n",
    "    \n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f9620",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns= extract_nouns(sents_with_krieg)\n",
    "nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27d9e9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"de_core_news_sm\")\n",
    "nouns=[]\n",
    "\n",
    "for index, row in hamburger_df.iterrows():\n",
    "    text= row[-2]\n",
    "    new_nouns=extract_nouns(text)\n",
    "    nouns.append(new_nouns) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef263148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network_data(nouns):\n",
    "    \"\"\"\n",
    "    converting a list of nouns into network data.\n",
    "    \"\"\"\n",
    "    \n",
    "    final_sources = []\n",
    "    final_targets = []\n",
    "    \n",
    "    for row in nouns:\n",
    "        source = row[0]\n",
    "        targets = row[1:]\n",
    "        for target in targets:\n",
    "            final_sources.append(source)\n",
    "            final_targets.append(target)\n",
    "    df = pd.DataFrame({'source':final_sources, 'target':final_targets})\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
