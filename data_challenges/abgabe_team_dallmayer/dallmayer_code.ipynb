{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Dallmayr\n",
    "\n",
    "## Participants\n",
    "- Lilian Do Khac\n",
    "- Daniel Thuerck\n",
    "- Boro Sofranac\n",
    "- Marc Mezger\n",
    "\n",
    "## Explanation\n",
    "This is the Juypter notebook to present our results. There is always a Markdown Cell explaning that is going to happen and then the code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading of the Project Files\n",
    "In this step the Pickle Files are downloaded\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Download the files\n",
    "import os\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "\n",
    "# Create 'downloaded' directory if it does not exist\n",
    "if not os.path.exists(\"downloaded\"):\n",
    "    os.makedirs(\"downloaded\")\n",
    "\n",
    "with open(\"data/hessenbox_link_list.txt\") as file:\n",
    "    for line in file:\n",
    "        # Skip the lines that start with '#'\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "\n",
    "        # Remove newline character from the end of line\n",
    "        line = line.strip()\n",
    "\n",
    "        # Download the file\n",
    "        filename = os.path.basename(line)\n",
    "\n",
    "        urllib.request.urlretrieve(urllib.parse.quote(line, safe=\":/\"), \"downloaded/\" + filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert the Pickle to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_pickle_file(file_path: Path) -> pd.DataFrame:\n",
    "    return pd.read_pickle(file_path)\n",
    "\n",
    "\n",
    "def process_directory(directory: Path, output_directory: Path) -> int:\n",
    "    files_list: List[Path] = [f for f in directory.iterdir() if f.is_file()]\n",
    "\n",
    "    for file_path in files_list:\n",
    "        df = load_pickle_file(file_path)\n",
    "        df = df[[\"paper_title\",\"publication_date\", \"plainpagefulltext\"]]\n",
    "        # Convert publication_date to string in ISO format\n",
    "        df[\"publication_date\"] = df[\"publication_date\"].dt.strftime(\"%Y-%m-%d\")\n",
    "        df = df[df['paper_title'].str.contains('Kölnische Zeitung', case=False, na=False)]\n",
    "\n",
    "        # Convert DataFrame to JSON\n",
    "        json_data = df.to_dict(orient=\"records\")\n",
    "\n",
    "        # Create output file path\n",
    "        output_file = output_directory / f\"{file_path.stem}.json\"\n",
    "\n",
    "        # Write JSON to file\n",
    "        with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    directory = Path(\"downloaded\")\n",
    "    output_directory = Path(\"jsons\")\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_directory.mkdir(exist_ok=True)\n",
    "\n",
    "    process_directory(directory, output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the files\n",
    "\n",
    "In this step the files are cleaned. For that purpose we used a selfhosted llm. The following variables were removed:     \n",
    "\"page_id\",\n",
    "    \"provider_ddb_id\",\n",
    "    \"provider\",\n",
    "    \"zdb_id\",\n",
    "    \"place_of_distribution\",\n",
    "    \"language\",\n",
    "    \"pagename\",\n",
    "    \"thumbnail\",\n",
    "    \"pagefulltext\",\n",
    "    \"preview_reference\"\n",
    "\n",
    "Prompt for cleaning:\n",
    "You are presented with some text in German from in between 1916 and 1945.\n",
    "It is written in the old German of that time. Please go through the text\n",
    "and adapt it to the modern german Language, removing spelling mistakes and\n",
    "grammar mistakes where appropriate. Try to recover the text as faithfully as possible,\n",
    "while minimizing the number of changes.\n",
    "Replace any character from old German language with their equivalents of today, e.g.\n",
    "\"ſ\" -> \"s\".\n",
    "\n",
    "Only return the corrected text, do not add any prose or explanations on what you did.\n",
    "\n",
    "Here is the text:\n",
    "{data}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pandas as pd\n",
    "import simplejson as json\n",
    "import pickle\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import ftfy\n",
    "import openai\n",
    "from dallmayr.prompts.prompt_loader import PromptLoader\n",
    "\n",
    "class DatetimeEncoder(json.JSONEncoder):\n",
    "\n",
    "  def default(self, obj):\n",
    "    try:\n",
    "      return super().default(obj)\n",
    "    except TypeError:\n",
    "      return str(obj)\n",
    "\n",
    "class Cleaner:\n",
    "\n",
    "  to_remove: List[str] = [\n",
    "    \"page_id\",\n",
    "    \"provider_ddb_id\",\n",
    "    \"provider\",\n",
    "    \"zdb_id\",\n",
    "    \"place_of_distribution\",\n",
    "    \"language\",\n",
    "    \"pagename\",\n",
    "    \"thumbnail\",\n",
    "    \"pagefulltext\",\n",
    "    \"preview_reference\"\n",
    "  ]\n",
    "\n",
    "  @staticmethod\n",
    "  def pd2json(data_path, files, year, paper_map):\n",
    "\n",
    "    # load all datafiles and concat all of them as\n",
    "    full_data = {t[1]: [] for t in paper_map.items()}\n",
    "    for file in files:\n",
    "      file_path = os.path.join(data_path, file)\n",
    "      df = pd.read_pickle(file_path)\n",
    "      json_data = df.to_dict(orient='records')\n",
    "\n",
    "      for record in json_data:\n",
    "        for key in Cleaner.to_remove:\n",
    "          del record[key]\n",
    "\n",
    "        for p_title, t_key in paper_map.items():\n",
    "          if p_title in record['paper_title']:\n",
    "            full_data[t_key].append(record)\n",
    "\n",
    "    # Save the JSON data to a file\n",
    "    for _, t_key in paper_map.items():\n",
    "      with open(f'{data_path}/json/{t_key}_{year}.json', 'w') as f:\n",
    "        json.dump(full_data[t_key], f, cls=DatetimeEncoder)\n",
    "\n",
    "  @staticmethod\n",
    "  def json2cleanjson(file_path, llm_model, llm_host):\n",
    "    # Initialize the OpenAI client\n",
    "    client_llm = openai.OpenAI(\n",
    "      api_key = \"None\",\n",
    "      base_url = f\"http://{llm_host}:5000/v1\"\n",
    "    )\n",
    "    client_embedding = openai.OpenAI(\n",
    "      api_key = \"None\",\n",
    "      base_url = f\"http://{llm_host}:5002/\"\n",
    "    )\n",
    "\n",
    "    # import data\n",
    "    with open(file_path, 'r', encoding=\"utf-8\") as f:\n",
    "      full_data = json.load(f)\n",
    "\n",
    "    def clean_record(record):\n",
    "      # fix encoding mistakes etc\n",
    "      full_text = ftfy.fix_text(record['plainpagefulltext'])\n",
    "\n",
    "      # cap so we don't exceed LLM size\n",
    "      last_ix = min(16384, len(full_text))\n",
    "      full_text = full_text[0:last_ix]\n",
    "\n",
    "      # use a small LLM to clean the text\n",
    "      prompt = PromptLoader.load(\"clean_ocr_data\").format(\n",
    "        data = full_text)\n",
    "\n",
    "      # Make a completion call\n",
    "      response = client_llm.chat.completions.create(\n",
    "        model = llm_model,\n",
    "        messages = [\n",
    "          {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "          {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        max_tokens=len(full_text),\n",
    "        temperature=0.9\n",
    "      )\n",
    "\n",
    "      # Extract the cleaned text from the response\n",
    "      cleaned_text = response.choices[0].message.content\n",
    "      record['cleantext'] = cleaned_text\n",
    "\n",
    "      # Compute the embedding\n",
    "      response = client_embedding.embeddings.create(\n",
    "        model = \"nomic-ai/nomic-embed-text-v1\",\n",
    "        input = full_text\n",
    "      )\n",
    "\n",
    "      # Extract the embedding vector from the response\n",
    "      embedding = response.data[0].embedding\n",
    "      record['embedding'] = embedding\n",
    "\n",
    "      return record\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "      # Submit the transform_item function for each item in the list\n",
    "      results = list(executor.map(clean_record, full_data))\n",
    "\n",
    "    # output cleaned data\n",
    "    clean_file_path = file_path.replace('.json', '_clean.json')\n",
    "    with open(clean_file_path, 'w', encoding=\"utf-8\") as f:\n",
    "      json.dump(results, f, cls=DatetimeEncoder)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting of the Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import time\n",
    "\n",
    "from dallmayr.cleaning.cleaner import Cleaner\n",
    "\n",
    "SCRIPT_PATH = os.path.dirname(os.path.realpath(__file__))\n",
    "DATA_PATH = os.path.join(SCRIPT_PATH, \"..\", \"..\", \"data\", \"downloaded\")\n",
    "JSON_PATH = os.path.join(SCRIPT_PATH, \"..\", \"..\", \"data\", \"downloaded\", \"json\")\n",
    "\n",
    "def main():\n",
    "\n",
    "  \n",
    "  files = os.listdir(JSON_PATH)\n",
    "\n",
    "  for file in files:\n",
    "    print(f\"Cleaning file: {file}\")\n",
    "\n",
    "    start = time.time()\n",
    "    Cleaner.json2cleanjson(os.path.join(JSON_PATH, file), \"llama3-small\", \n",
    "      \"localhost\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"> Completed in {end - start} seconds.\")\n",
    "\n",
    "    exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the newspaper articles\n",
    "In the next steps the articles are summorized using Googles Gemini Flash Model. This allows to reduce the amount of text while still keeping the important information. This was done for all of the selected newspaper articles. Which were always the first page of the newspapers. We selected only three newspapers to allow for a faster development with lower costs for large language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, HarmBlockThreshold, HarmCategory\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "safety_settings = {\n",
    "    HarmCategory.HARM_CATEGORY_UNSPECIFIED: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "}\n",
    "\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\", temperature=0, max_tokens=200, timeout=None, max_retries=50, safety_settings=safety_settings\n",
    ")  # .with_structured_output(result_schema)\n",
    "\n",
    "# load the data\n",
    "sueddeutsche = pd.read_pickle(\"exported_data/suddeutsche.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"human\",\n",
    "        \"\"\"Input: {{text}}\n",
    "\n",
    "You will generate topics and dense summaries of the topics of the above collection of articles.\n",
    "\n",
    "Execute the following 2 steps.\n",
    "\n",
    "Step 1. Identify informative topics from the collection of Articles.\n",
    "Step 2. Write a summaries of identical length which covers every entity.\n",
    "\n",
    "An Entity is:\n",
    "\n",
    "Specific: descriptive yet concise (5 words or fewer).\n",
    "Faithful: present in the Articles.\n",
    "Anywhere: located anywhere in the Articles.\n",
    "\n",
    "Guidelines:\n",
    "\n",
    "The summary should be long (4-5 sentences, ~80 words)\n",
    "The summaries should become highly dense and concise yet self-contained, e.g., easily understood without the Articles.\n",
    "Replace old German characters (e.g. ſ) with modern German characters.\n",
    "\n",
    "Remember, use the exact same number of words for each summary.\n",
    "\n",
    "Answer in German with a JSON. The JSON should be a list of dictionaries whose keys are \"Topic\", \"Keywords\", \"Persons\", \"Organizations\", \"Sentiment\", \"Objects\", \"Technology\" and \"Summary\".\"\"\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages(messages, template_format=\"jinja2\")\n",
    "\n",
    "sueddeutsche = sueddeutsche.sort_values(by=\"publication_date\")\n",
    "\n",
    "sueddeutsche = sueddeutsche.iloc[::50, :]\n",
    "\n",
    "chain = template | llm\n",
    "result_list = []\n",
    "\n",
    "\n",
    "def chunker(seq, size):\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq.iloc[pos : pos + size]\n",
    "\n",
    "\n",
    "chunk_size = 30\n",
    "for i in tqdm(chunker(sueddeutsche, chunk_size)):\n",
    "    texts = [{\"text\": t.plainpagefulltext} for t in i.itertuples()]\n",
    "    \n",
    "    # save the original text as txt file\n",
    "    for t in i.itertuples():\n",
    "        with open(\n",
    "            f\"original_sued/{t.publication_date.year}_sued_{t.publication_date.day}-{t.publication_date.month}-{t.publication_date.year}_{t.pagenumber!s}.txt\", \"w\"\n",
    "        ) as f:\n",
    "            f.write(t.plainpagefulltext)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedd and classify the sentiment\n",
    "\n",
    "Now where are Embedding the Texts with Google Text Embedding 004 and the clustering setting. \n",
    "Clustering embedded texts involves grouping similar texts together based on their meaning, as represented by their embeddings. Embeddings are numerical vector representations of text where semantically similar texts have vectors that are close together in the vector space. This allows for more nuanced clustering than simply looking at keywords or word counts. In addition we classify the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "# Initialize the a specific Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"text-embedding-004\")\n",
    "\n",
    "\n",
    "output_directory = Path(\"embeddings\")\n",
    "output_directory.mkdir(exist_ok=True)  # Ensure the output directory exists\n",
    "input_directory = Path(\"jsons\")  # Define your input directory path\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Loop over the JSON files in the input folder\n",
    "    for f in input_directory.glob(\"*.json\"):\n",
    "        # Load JSON file\n",
    "        with open(f, \"r\", encoding=\"utf-8\") as ff:\n",
    "            data = json.load(ff)\n",
    "\n",
    "        # Loop over the objects in the JSON file\n",
    "        for idx, json_obj in enumerate(data):\n",
    "            # Generate embeddings for each object\n",
    "            embeddings = embeddings.embed_text(prompt=json_obj[\"plainpagefulltext\"], task=\"clustering\")\n",
    "            # Convert embeddings to NumPy array\n",
    "            embeddings_array = np.array(embeddings)\n",
    "            # Save embeddings to a compressed NumPy file with a unique name\n",
    "            output_file = output_directory / f\"{f.stem}_{idx}.npz\"\n",
    "            np.savez_compressed(output_file, embeddings=embeddings_array)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Afterwards we are clustering with KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import cohere\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "import seaborn as sns\n",
    "from kneed import KneeLocator\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "k = 10\n",
    "\n",
    "def cluster_embeddings(embeddings, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    return cluster_labels\n",
    "\n",
    "def main():\n",
    "    # Embedding of texts\n",
    "    \n",
    "    files = list(Path(\"embeddings_sentiment\").rglob(\"*.json\"))\n",
    "\n",
    "    file_contents = []\n",
    "    embeddings = []\n",
    "    years = []\n",
    "    newspaper = []\n",
    "    days = []\n",
    "    months = []\n",
    "    texts=[]\n",
    "    dates = []\n",
    "    sentiments = []\n",
    "    for f in files:\n",
    "        with f.open() as file:\n",
    "            content = json.load(file)\n",
    "            file_contents.append(content)\n",
    "            for c in content:\n",
    "                years.append(int(f.name.split(\"_\")[0]))\n",
    "                newspaper.append(f.name.split(\"_\")[1])\n",
    "                days.append(int(f.name.split(\"_\")[2].split(\"-\")[0]))\n",
    "                months.append(int(f.name.split(\"_\")[2].split(\"-\")[1]))\n",
    "                texts.append(c[\"content\"])\n",
    "                embeddings.append(c[\"embedding\"])\n",
    "                sentiments.append(c[\"sentiment\"])\n",
    "                dates.append(datetime.datetime(int(f.name.split(\"_\")[0]),int(f.name.split(\"_\")[2].split(\"-\")[1]),int(f.name.split(\"_\")[2].split(\"-\")[0] )))\n",
    "\n",
    "    data = {\"day\": days, \"month\":months, \"year\": years, \"newspaper\": newspaper, \"embeddings\": embeddings, \"text\": texts, \"date\": dates, \"sentiment\": sentiments}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[(df['year'] >= 1914) & (df['year'] <= 1919)]\n",
    "    df = df.drop(df[~(df['sentiment'] == 'neutral')].index)\n",
    "    #df = df.drop(df[~(df['year'] == 1939)].index)\n",
    "    #df.drop(df[~(df['year'] == 1939)].index)\n",
    "\n",
    "    # Clustering of embeddings\n",
    "    cluster_labels = cluster_embeddings(list(df[\"embeddings\"]), k)\n",
    "\n",
    "    print(\"asdf\")\n",
    "    #df1.to_excel(\"results.xlsx\")\n",
    "    df[\"cluster labels\"] = cluster_labels\n",
    "    df.to_csv(\"results.csv\", sep=\";\")\n",
    "\n",
    "    # ------------------------------------------------------------- Visualization 1\n",
    "    list_of_arrays = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        list_of_arrays.append(np.array(row[\"embeddings\"]))\n",
    "\n",
    "    my_array = np.array(list_of_arrays)\n",
    "\n",
    "    pca = PCA(n_components=3)\n",
    "    pca.fit(my_array) \n",
    "    X_pca = pca.transform(my_array) \n",
    "\n",
    "    total_var = pca.explained_variance_ratio_.sum() * 100\n",
    "\n",
    "    fig = px.scatter_3d(\n",
    "        X_pca,\n",
    "        #x=df[\"newspaper\"], y=df[\"sentiment\"], z=df[\"year\"],  \n",
    "        x=0, y=1, z=2, \n",
    "        color=cluster_labels,\n",
    "        opacity=0.5,\n",
    "        title=f'Neutral Sentiment 1914-1919',\n",
    "        labels=cluster_labels,\n",
    "        #df[\"sentiment\"]\n",
    "    )\n",
    "    fig.update_traces(marker_size = 4)\n",
    "    fig.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization of the Reflection with Google Gemini\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from loguru import logger\n",
    "\n",
    "k = 10\n",
    "\n",
    "\n",
    "def cluster_embeddings(embeddings, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    kmeans.fit(embeddings)\n",
    "    cluster_labels = kmeans.labels_\n",
    "    return cluster_labels\n",
    "\n",
    "\n",
    "def pre():\n",
    "    # Embedding of texts\n",
    "\n",
    "    files = list(Path(\"json/embeddings_sentiment\").rglob(\"*.json\"))\n",
    "\n",
    "    file_contents = []\n",
    "    embeddings = []\n",
    "    years = []\n",
    "    newspaper = []\n",
    "    days = []\n",
    "    months = []\n",
    "    texts = []\n",
    "    dates = []\n",
    "    titles = []\n",
    "    sentiments = []\n",
    "    for f in files:\n",
    "        with f.open() as file:\n",
    "            content = json.load(file)\n",
    "            file_contents.append(content)\n",
    "            for c in content:\n",
    "                years.append(int(f.name.split(\"_\")[0]))\n",
    "                newspaper.append(f.name.split(\"_\")[1])\n",
    "                days.append(int(f.name.split(\"_\")[2].split(\"-\")[0]))\n",
    "                months.append(int(f.name.split(\"_\")[2].split(\"-\")[1]))\n",
    "                texts.append(c[\"content\"])\n",
    "                embeddings.append(c[\"embedding\"])\n",
    "                sentiments.append(c[\"sentiment\"])\n",
    "                titles.append(c[\"title\"])\n",
    "                dates.append(datetime.datetime(int(f.name.split(\"_\")[0]), int(f.name.split(\"_\")[2].split(\"-\")[1]), int(f.name.split(\"_\")[2].split(\"-\")[0])))\n",
    "\n",
    "    data = {\"day\": days, \"month\": months, \"year\": years, \"newspaper\": newspaper, \"embeddings\": embeddings, \"text\": texts, \"date\": dates, \"sentiment\": sentiments, \"title\": titles}\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[(df[\"year\"] >= 1914) & (df[\"year\"] <= 1919)]\n",
    "    df = df.drop(df[~(df[\"sentiment\"] == \"positive\")].index)\n",
    "    cluster_labels = cluster_embeddings(list(df[\"embeddings\"]), k)\n",
    "\n",
    "    # df1.to_excel(\"results.xlsx\")\n",
    "    df[\"cluster labels\"] = cluster_labels\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "prompt = \"\"\"\n",
    "How did German Newspapers reflect postive sentiments about technological advancements and what factors influenced these perceptions? Newspaper articles:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def gen(text: str):\n",
    "    response = model.generate_content(prompt+text)\n",
    "    return response.text\n",
    "\n",
    "def main():\n",
    "    df = pre()\n",
    "    results = []\n",
    "\n",
    "    for i in range(10):\n",
    "        # group by cluster\n",
    "        cluster = df[df[\"cluster labels\"] == i]\n",
    "\n",
    "        contents = []\n",
    "        for idx, row in cluster.iterrows():\n",
    "            contents.append(\"Titel:\" + row[\"title\"] + \"\\n\" + \"Text\" + row[\"text\"] + \"\\n\")\n",
    "\n",
    "        text = \" \".join(contents)\n",
    "\n",
    "        result = gen(text)\n",
    "\n",
    "        logger.info(f\"Result for cluster {i}: {result}\")\n",
    "        results.append(result)\n",
    "\n",
    "\n",
    "    # save results as text with cluster number\n",
    "    with open(\"results.txt\", \"w\") as f:\n",
    "        for i, r in enumerate(results):\n",
    "            f.write(f\"Cluster {i}:\\n\")\n",
    "            f.write(r)\n",
    "            f.write(\"\\n\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now letting the llm reflect how it changed from 1914 to 1944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from loguru import logger\n",
    "\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro-002\")\n",
    "prompt = \"\"\"\n",
    "Du bekommst zwei text inputs der erste ist ovn 1914-1919 und der zweite von 1938-1944. \n",
    "How did German Newspapers reflect postive sentiments about technological advancements and what factors influenced these perceptions? Newspaper articles:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def gen(text: str):\n",
    "    response = model.generate_content(prompt + text)\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def main():\n",
    "    # open the txt files 1914 and 1944 and sent it to the model\n",
    "    file1914 = Path(\"results19141918-positive.txt\")\n",
    "    file1944 = Path(\"results1944-positive.txt\")\n",
    "\n",
    "    with file1914.open() as f:\n",
    "        text1914 = f.read()\n",
    "\n",
    "    with file1944.open() as f:\n",
    "        text1944 = f.read()\n",
    "\n",
    "\n",
    "    # combine the two texts\n",
    "    combined_text = \"die artikel von 1914-1919\"+text1914 + \"\\n\\n\\n\" + \"die artikel von 1938-1945\"+text1944\n",
    "\n",
    "    results = gen(combined_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results Postive Reflection\n",
    "\n",
    "Zwischen 1914-1919 und 1938-1945 verändert sich die Darstellung und Wahrnehmung technologischer Fortschritte in deutschen Zeitungen deutlich.  Hier eine Zusammenfassung der wichtigsten Unterschiede:\n",
    "\n",
    "**1914-1919 (Erster Weltkrieg und unmittelbare Nachkriegszeit):**\n",
    "\n",
    "* **Indirekte positive Darstellung:** Technologie wird nicht direkt gepriesen, sondern im Kontext von positiven Folgen wie internationale Kooperation, wirtschaftliche Erholung und nationale Einheit.  Der Fokus liegt auf den *Ergebnissen* des technologischen Fortschritts, nicht auf der Technologie selbst.\n",
    "* **Diplomatie und Wirtschaft im Vordergrund:**  Die Artikel betonen die Rolle von Technologien wie Telegraf, Zug und Dampfschiff bei der Ermöglichung von Diplomatie, Handel und internationalen Verträgen.\n",
    "* **Propaganda und Moralerhaltung:** Die positive Darstellung dient der Moralerhaltung und der Rechtfertigung der Kriegsanstrengungen.  Negative Aspekte der Technologie, insbesondere ihre zerstörerische Kraft im Krieg, werden heruntergespielt.\n",
    "* **Implizite Anerkennung wissenschaftlichen Fortschritts:**  In einigen Artikeln wird die Bedeutung wissenschaftlicher Forschung und Entwicklung angedeutet, jedoch ohne detaillierte technische Beschreibungen.\n",
    "\n",
    "**1938-1945 (Zeit des Nationalsozialismus und Zweiter Weltkrieg):**\n",
    "\n",
    "* **Direkte und überschwängliche positive Darstellung:** Technologischer Fortschritt, insbesondere in Bereichen wie Luftfahrt, Automobilbau und Kommunikation, wird explizit gepriesen und als Beweis deutscher Überlegenheit dargestellt.\n",
    "* **Nationalismus und Propaganda:** Die Artikel sind stark nationalistisch geprägt und dienen der Propaganda des NS-Regimes.  Technologische Errungenschaften werden als Zeichen nationaler Stärke und Größe inszeniert.\n",
    "* **Fokus auf Rekorde und Siege:**  Die Berichterstattung konzentriert sich auf Rekorde, Siege in Rennen und sportliche Triumphe, die die deutsche technische Leistungsfähigkeit demonstrieren sollen.\n",
    "* **Verherrlichung militärischer Technologie:** Im Kontext des Zweiten Weltkriegs wird die militärische Technologie, insbesondere die Luftwaffe und U-Boote, glorifiziert.\n",
    "* **Ausblendung negativer Aspekte und der Konkurrenz:**  Die Leistungen anderer Nationen werden heruntergespielt, und negative Aspekte oder Herausforderungen der deutschen Technologie werden ausgeblendet.\n",
    "* **Infrastruktur und Autarkie:** Technologischer Fortschritt wird mit der Stärkung der Infrastruktur und dem Streben nach Autarkie verknüpft.\n",
    "\n",
    "**Zusammenfassend lässt sich sagen:**\n",
    "\n",
    "Die Wahrnehmung technologischen Fortschritts wandelt sich von einer indirekten, kontextbezogenen und propagandistisch geprägten Darstellung im Ersten Weltkrieg zu einer direkten, überschwänglichen und nationalistisch aufgeladenen Verherrlichung im Nationalsozialismus. Der Fokus verschiebt sich von den positiven Folgen der Technologie auf die Technologie selbst, insbesondere im militärischen Bereich.  Die Berichterstattung wird zunehmend propagandistisch und dient der Stärkung des NS-Regimes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Negative Reflection\n",
    "Die größte Veränderung zwischen 1914-1919 und 1938-1945 ist der **Wechsel vom Ersten zum Zweiten Weltkrieg und der damit einhergehende Aufstieg des Nationalsozialismus in Deutschland**.  Diese Veränderung prägt die Sichtweise auf technologischen Fortschritt in den Zeitungsartikeln fundamental.\n",
    "\n",
    "**1914-1919:**\n",
    "\n",
    "* **Desillusionierung durch den Krieg:**  Die Erfahrung des Ersten Weltkriegs, insbesondere die verheerenden Folgen neuer Waffentechnologien wie Giftgas und Maschinengewehre, führten zu einer Ernüchterung über den technologischen Fortschritt.  Technologie wurde mit Leid, Zerstörung und dem Scheitern der Kriegsziele assoziiert.\n",
    "* **Soziale und wirtschaftliche Folgen:** Die Kriegswirtschaft, Ressourcenknappheit und soziale Umwälzungen wurden ebenfalls mit Technologie in Verbindung gebracht, allerdings weniger direkt.  Es gab Ängste vor den Folgen der Industrialisierung und der Veränderung der Arbeitswelt.\n",
    "* **Propaganda und Medien:** Die zunehmende Bedeutung von Massenmedien und Propaganda wurde als zweischneidiges Schwert wahrgenommen. Technologie ermöglichte die Verbreitung von Informationen, aber auch deren Manipulation.\n",
    "\n",
    "**1938-1945:**\n",
    "\n",
    "* **Instrumentalisierung der Technologie:** Unter dem Nationalsozialismus wurde Technologie gezielt für die Kriegsführung und die Durchsetzung der Ideologie instrumentalisiert.  Technologischer Fortschritt wurde vor allem im militärischen Bereich vorangetrieben, während zivile Innovationen in den Hintergrund traten.\n",
    "* **Kontrolle und Zensur:**  Die Nazis kontrollierten die Medien und unterdrückten kritische Stimmen.  Technologien der Kommunikation wurden für Propaganda und die Verbreitung der NS-Ideologie genutzt.\n",
    "* **Wirtschaftliche Regulierung:** Die Wirtschaft wurde streng reguliert und auf den Krieg ausgerichtet.  Technologischer Fortschritt wurde durch Ressourcenknappheit und die Fokussierung auf Rüstungsproduktion gehemmt.\n",
    "* **Feindbild und Propaganda:** Technologische Überlegenheit der Alliierten wurde heruntergespielt oder als barbarisch dargestellt.  Deutsche \"Wunderwaffen\" wurden propagandistisch hervorgehoben, um die Moral zu stärken.\n",
    "\n",
    "**Zusammenfassend lässt sich sagen:**\n",
    "\n",
    "Während im Ersten Weltkrieg eine generelle Desillusionierung über den technologischen Fortschritt herrschte, der mit Zerstörung und Leid verbunden wurde, wurde Technologie im Zweiten Weltkrieg unter den Nazis gezielt instrumentalisiert und kontrolliert.  Die Propaganda nutzte die Technologie zur Verbreitung der Ideologie und zur Manipulation der öffentlichen Meinung.  Die negative Wahrnehmung von Technologie konzentrierte sich dabei vor allem auf die technologische Überlegenheit der Feinde und die damit verbundene Bedrohung.  Die Kontrolle und Lenkung der Technologie durch das NS-Regime hemmte zudem den zivilen technologischen Fortschritt.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
